{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "\n",
    "from blackhc.mdp import dsl\n",
    "from blackhc import mdp\n",
    "import time\n",
    "\n",
    "from blackhc.mdp import lp\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def smooth_check(A,a):\n",
    "    # print(\"inside\")\n",
    "    # print(a)\n",
    "    # print(\"out\")\n",
    "    for ele in A.keys():\n",
    "        # print(ele)\n",
    "        if torch.eq(a,ele).all():\n",
    "            # print(\"Here\")\n",
    "            return ele\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_env:\n",
    "    def __init__(self,horizon_length):\n",
    "        \n",
    "        self.prev_state =None\n",
    "        self.curr_state =None\n",
    "           \n",
    "        self.state_list=list()\n",
    "        self.action_list=list()\n",
    "        self.state_to_action_map=dict()\n",
    "\n",
    "        \n",
    "        self.P=defaultdict()\n",
    "        self.R=defaultdict()\n",
    "        self.horizon_len=horizon_length\n",
    "        self.terminal_state=None\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.state_list=list()\n",
    "        self.action_list=list()\n",
    "        self.state_to_action_map=dict()\n",
    "\n",
    "        \n",
    "        self.P=defaultdict()\n",
    "        self.R=defaultdict()\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def update_param_given_epi(self,episodes):\n",
    "        \n",
    "\n",
    "        # following SARSA format\n",
    "        for epi_id in range(len(episodes)):\n",
    "    \n",
    "            t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(episodes[epi_id])\n",
    "            # print(episodes[epi_id])\n",
    "            # print(t_states)\n",
    "            # print(t_actions)\n",
    "            # print(t_rewards)\n",
    "            # print(t_nstates)\n",
    "            # print(t_log_probs)\n",
    "            # print(\"Im here\")\n",
    "            i=0 \n",
    "            while i<len(t_states):\n",
    "                \n",
    "                # updating the list of states\n",
    "                \n",
    "                if any([torch.equal(x,t_states[i]) for x in self.state_list])!=True:\n",
    "                    self.state_list.append(t_states[i])\n",
    "                    self.state_to_action_map.update({t_states[i]:[]})\n",
    "                \n",
    "                \n",
    "                \n",
    "                # updating the list of actions\n",
    "            \n",
    "            \n",
    "\n",
    "                if any([torch.equal(x,t_actions[i]) for x in self.action_list])!=True:\n",
    "                    self.action_list.append(t_actions[i])\n",
    "                    \n",
    "                if any([torch.equal(x,t_actions[i]) for x in self.state_to_action_map[smooth_check(self.state_to_action_map,t_states[i])]])!=True:\n",
    "                    self.state_to_action_map[smooth_check(self.state_to_action_map,t_states[i])].append(t_actions[i])\n",
    "                \n",
    "                # # update state,action to next state count map\n",
    "                \n",
    "                tru_tup,flag=self.double_smooth_check(self.P,(t_states[i],t_actions[i]))\n",
    "                if flag!=True:\n",
    "                    self.P[(t_states[i],t_actions[i])]={t_nstates[i]:1}\n",
    "                    self.R[(t_states[i],t_actions[i])]={t_nstates[i]:t_rewards[i]}\n",
    "                    \n",
    "                else:\n",
    "                    if any([torch.equal(x,t_nstates[i]) for x in self.P[tru_tup].keys()])!=True:\n",
    "                        self.P[tru_tup].update({t_nstates[i]:1})\n",
    "                        self.R[tru_tup].update({t_nstates[i]:t_rewards[i]})\n",
    "                        \n",
    "                    else:\n",
    "                        sec_tup=smooth_check(self.P[tru_tup],t_nstates[i])\n",
    "                        self.P[tru_tup][sec_tup]+=1\n",
    "                                    \n",
    "                i+=1  \n",
    "                \n",
    "            if self.terminal_state is None and i<self.horizon_len:\n",
    "                self.terminal_state=t_nstates[i-1]\n",
    "                self.state_list.append(t_nstates[i-1])\n",
    "        return \n",
    "    \n",
    "\n",
    "    def double_smooth_check(self,A,a):\n",
    "        \n",
    "        for ele in A.keys():\n",
    "            if torch.equal(a[0],ele[0]) and torch.equal(a[1],ele[1]):\n",
    "                return ele,True\n",
    "        return a,False\n",
    "    \n",
    "    \n",
    "    \n",
    "    def cvt_axis(self,traj):\n",
    "        \n",
    "        t_states =[]\n",
    "        t_actions =[]\n",
    "        t_nstates =[]\n",
    "        t_rewards=[]\n",
    "        t_log_probs=[]\n",
    "        \n",
    "        for i in range(len(traj[0])):\n",
    "            t_states.append(traj[0][i])\n",
    "            t_actions.append(traj[1][i])\n",
    "            t_rewards.append(traj[2][i])\n",
    "            t_nstates.append(traj[3][i])\n",
    "            t_log_probs.append(traj[4][i])\n",
    "\n",
    "        return (t_states, t_actions, t_rewards,t_nstates,t_log_probs) \n",
    "    \n",
    "    def get_parameters(self):\n",
    "        print(\"\\nState list\")\n",
    "        print(self.state_list)\n",
    "        print(\"\\nAction list\")\n",
    "        print(self.action_list)\n",
    "        print(\"\\nState to action map\")\n",
    "        print(self.state_to_action_map)\n",
    "        print(\"\\nstate_action to next state\")\n",
    "        for x in self.P:\n",
    "            print(x)\n",
    "            print(self.P[x])\n",
    "        print(\"\\n state_action to reward map\")\n",
    "        for x in self.R:\n",
    "            print(x)\n",
    "            print(self.R[x])\n",
    "    \n",
    "    def update_D_fake(num_of_traj,D_real,D_fake):\n",
    "        # sample a state from D_real\n",
    "        return \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
