{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "\n",
    "from blackhc.mdp import dsl\n",
    "from blackhc import mdp\n",
    "import time\n",
    "\n",
    "from blackhc.mdp import lp\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    def flush_all(self):\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        return\n",
    "\n",
    "    def push(self, state, action, reward, next_state,policy):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state,policy)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def push_batch(self, batch):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            append_len = min(self.capacity - len(self.buffer), len(batch))\n",
    "            self.buffer.extend([None] * append_len)\n",
    "\n",
    "        if self.position + len(batch) < self.capacity:\n",
    "            self.buffer[self.position : self.position + len(batch)] = batch\n",
    "            self.position += len(batch)\n",
    "        else:\n",
    "            self.buffer[self.position : len(self.buffer)] = batch[:len(self.buffer) - self.position]\n",
    "            self.buffer[:len(batch) - len(self.buffer) + self.position] = batch[len(self.buffer) - self.position:]\n",
    "            self.position = len(batch) - len(self.buffer) + self.position\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        batch = random.sample(self.buffer, int(batch_size))\n",
    "        state, action, reward, next_state,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state,policy\n",
    "\n",
    "    def sample_all_batch(self, batch_size):\n",
    "        idxes = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = list(itemgetter(*idxes)(self.buffer))\n",
    "        state, action, reward, next_state,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state,policy\n",
    "\n",
    "    def return_all(self):\n",
    "        return self.buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption : The reward function is known prior:\n",
    "import itertools\n",
    "\n",
    "class A_model:\n",
    "    def __init__(self,num_of_states,num_of_actions,prior_vec):\n",
    "        self.states = np.arange(num_of_states)\n",
    "        self.actions = np.arange(num_of_actions)\n",
    "        self.terminal_state = None\n",
    "        self.state_action_to_state_dict=dict()\n",
    "        self.state_action_to_reward_dict=dict()\n",
    "        self.prior_vec = prior_vec\n",
    "        \n",
    "        self.curr_state = None\n",
    "        self.horizon_len=20 # this specifies the horizon_len for updating D_real\n",
    "        self.last_seen_len=0 # assuming D_real with infinit capacity\n",
    "        self.k=10 # specifies the horizon len for updating_len for updating D_fake  \n",
    "        \n",
    "    def configure(self):\n",
    "        # Initialisng the transition prob_matrix\n",
    "        key_list=list(itertools.product(self.states, self.actions))\n",
    "        for x in key_list:\n",
    "            self.state_action_to_state_dict.update({x:dict()})\n",
    "            for y in range(len(self.states)):\n",
    "                self.state_action_to_state_dict[x].update({self.states[y]:self.prior_vec[y]})\n",
    "        # init the rewrad matrix\n",
    "        \n",
    "        for x in key_list:\n",
    "            self.state_action_to_reward_dict.update({x:dict()})\n",
    "            for y in range(len(self.states)):\n",
    "                if self.states[y]==self.terminal_state:\n",
    "                    self.state_action_to_reward_dict[x].update({self.states[y]:np.random.random(1).item()})\n",
    "                else:\n",
    "                    self.state_action_to_reward_dict[x].update({self.states[y]:np.random.random(1).item()})\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state_action_to_state_dict=dict()\n",
    "        self.state_action_to_reward_dict=dict()\n",
    "        self.last_seen_len=0\n",
    "    \n",
    "    def update_param_given_epi(self,D_real):\n",
    "        \n",
    "        episodes=D_real.buffer[self.last_seen_len:len(D_real.buffer)]\n",
    "        self.last_seen_len=len(D_real.buffer)\n",
    "\n",
    "        # print(len(episodes))\n",
    "        \n",
    "        \n",
    "        # following SARSA format\n",
    "        for epi_id in range(len(episodes)):\n",
    "    \n",
    "            t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(episodes[epi_id])\n",
    "            i=0 \n",
    "            \n",
    "            while i<len(t_states):\n",
    "                \n",
    "                # updating the list of states\n",
    "                \n",
    "                # tru_tup=self.state_action_to_state_dict[(t_states[i],t_actions[i].item())]\n",
    "\n",
    "\n",
    "                self.state_action_to_state_dict[(t_states[i],t_actions[i].item())][t_nstates[i]]+=1\n",
    "                self.state_action_to_reward_dict[(t_states[i],t_actions[i].item())][t_nstates[i]]=t_rewards[i]\n",
    "                                    \n",
    "                i+=1  \n",
    "                \n",
    "            if self.terminal_state is None and i<self.horizon_len:\n",
    "                self.terminal_state=t_nstates[i-1]\n",
    "                self.state_list.append(t_nstates[i-1])\n",
    "        return\n",
    "\n",
    "    def cvt_axis(self,traj):\n",
    "        \n",
    "        t_states =[]\n",
    "        t_actions =[]\n",
    "        t_nstates =[]\n",
    "        t_rewards=[]\n",
    "        t_log_probs=[]\n",
    "        \n",
    "        for i in range(len(traj[0])):\n",
    "            t_states.append(traj[0][i])\n",
    "            t_actions.append(traj[1][i])\n",
    "            t_rewards.append(traj[2][i])\n",
    "            t_nstates.append(traj[3][i])\n",
    "            t_log_probs.append(traj[4][i])\n",
    "\n",
    "        return (t_states, t_actions, t_rewards,t_nstates,t_log_probs) \n",
    "    \n",
    "    def step(self,a_t):\n",
    "        \n",
    "        next_state=0\n",
    "        un_norm_distr=self.state_action_to_state_dict[(self.curr_state,a_t.item())]\n",
    "        norm_factor=sum(list(un_norm_distr.values()))\n",
    "        choices=list(un_norm_distr.keys())\n",
    "        p=[x/norm_factor for x in un_norm_distr.values()]\n",
    "        \n",
    "        next_state_id=np.random.choice(np.arange(len(choices)),p=p)\n",
    "        next_state=choices[next_state_id]\n",
    "        reward=self.state_action_to_reward_dict[(self.curr_state,a_t.item())][next_state]\n",
    "\n",
    "        self.curr_state=next_state\n",
    "        Is_done=False\n",
    "        if self.terminal_state==next_state:\n",
    "            Is_done=True\n",
    "\n",
    "        return next_state,reward,Is_done,None\n",
    "\n",
    "    def set_start_state(self):\n",
    "        self.curr_state=np.random.choice(self.states)\n",
    "        while self.curr_state==self.terminal_state:\n",
    "            self.curr_state=np.random.choice(self.states)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  _multi_round_nmdp_simple():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(0) | dsl.reward(10)\n",
    "        start & A_0 > start * 10 | end\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_1 > start * 10 | end * 1 | S_1 * 1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(10)\n",
    "        S_1 & A_1 > start * 5 | end\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate()\n",
    "    \n",
    "def  _multi_round_nmdp_complex():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        S_2=dsl.state()\n",
    "        S_3=dsl.state()\n",
    "        S_4=dsl.state()\n",
    "        S_5=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_0 > end * 1 | start\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        start & A_1 > start * 1 | S_1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_1 > S_1 * 1 | S_2\n",
    "        \n",
    "        S_2 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_0 > S_2 * 1 | S_1\n",
    "        S_2 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_1 > S_2 * 1 | S_3\n",
    "        \n",
    "        S_3 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_0 > S_3 * 1 | S_2\n",
    "        S_3 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_1 > S_3 * 1 | S_4\n",
    "        \n",
    "        S_4 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_0 > S_4 * 1 | S_3\n",
    "        S_4 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_1 > S_4 * 1 | S_5\n",
    "        \n",
    "        S_5 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_5 & A_0 > S_5 * 1 | S_4\n",
    "        S_5 & A_1 > dsl.reward(10) | dsl.reward(0)\n",
    "        S_5 & A_1 > end * 1 | S_1\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "\n",
    "def  _multi_round_nmdp_complex_v1():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        # S_0=dsl.state()\n",
    "        # S_1=dsl.state()\n",
    "        # S_2=dsl.state()\n",
    "        # S_3=dsl.state()\n",
    "        # S_4=dsl.state()\n",
    "        # S_5=dsl.state()\n",
    "        \n",
    "        state_list=[]\n",
    "        num_of_states_with_term=10\n",
    "        for i in range(num_of_states_with_term):\n",
    "            if i!=num_of_states_with_term-1:\n",
    "                state_list.append(dsl.state())\n",
    "            else:\n",
    "                state_list.append(dsl.terminal_state())\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        state_list[0] & A_0 > dsl.reward(-1) \n",
    "        state_list[0] & A_0 > state_list[0]\n",
    "        state_list[0] & A_1 > dsl.reward(-1) | dsl.reward(0)\n",
    "        state_list[0] & A_1 > state_list[0] * 1 | state_list[1] * 1\n",
    "        \n",
    "        for  i in range(1,num_of_states_with_term-1):\n",
    "            state_list[i] & A_0 > dsl.reward(-1) | dsl.reward(0)\n",
    "            state_list[i] & A_0 > state_list[i-1] * 1 | state_list[i] * 1 \n",
    "            if i+1 != 6:\n",
    "                state_list[i] & A_1 > dsl.reward(-1) | dsl.reward(0+i*0.1)\n",
    "                state_list[i] & A_1 > state_list[i] * 1 | state_list[i+1] * 1 \n",
    "            else:\n",
    "                state_list[i] & A_1 > dsl.reward(-1) | dsl.reward(10)\n",
    "                state_list[i] & A_1 > state_list[i] * 1 | state_list[i+1] * 1 \n",
    "        \n",
    "        \n",
    "        dsl.discount(1)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "\n",
    "\n",
    "MULTI_ROUND_NDMP = _multi_round_nmdp_complex_v1()\n",
    "\n",
    "\n",
    "\n",
    "solver = lp.LinearProgramming(MULTI_ROUND_NDMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = MULTI_ROUND_NDMP.to_env()\n",
    "# # state=env.observation_space.n\n",
    "# # print(state)\n",
    "# env.reset()\n",
    "# is_done=False\n",
    "# counter = 0\n",
    "# while not is_done:\n",
    "#     # print(state)\n",
    "#     act=env.action_space.sample()\n",
    "#     # print(act)\n",
    "#     state, reward, is_done, _ = env.step(act)\n",
    "#     # print(state)\n",
    "#     # print(reward)\n",
    "#     counter+=1\n",
    "# # print(env.observation_space.n)\n",
    "# # print(env.action_space.n)\n",
    "# print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# update D_real\n",
    "\n",
    "\n",
    "def update_D_real(D_real,env,Agent,num_of_epochs):\n",
    "    \n",
    "    A1=Agent\n",
    "    \n",
    "    horizon_len=40\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    s_t_index=env._state.index\n",
    "\n",
    "    trajs=[]\n",
    "    # D_real.flush_all()\n",
    "\n",
    "    result=0\n",
    "\n",
    "\n",
    "    for traj_id in range(num_of_epochs):\n",
    "        env.reset()\n",
    "        # print(env._state.index)\n",
    "        # display_env()\n",
    "        s_t_index=env._state.index\n",
    "        \n",
    "        states=[]\n",
    "        log_probs=[]\n",
    "        rewards=[]\n",
    "        actions=[]\n",
    "        nstates=[]\n",
    "        \n",
    "        for t in range(horizon_len):\n",
    "            \n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            a_t, log_prob = A1.action(s_t)\n",
    "            ns_t_index, r_t, done, _ = env.step(a_t.numpy()[0][0])\n",
    "            \n",
    "            states.append(s_t_index)\n",
    "            actions.append(a_t)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(r_t)\n",
    "            nstates.append(ns_t_index)\n",
    "            s_t_index=ns_t_index\n",
    "            if done:\n",
    "                break   \n",
    "        D_real.push(states, actions, rewards,nstates, log_probs)\n",
    "    \n",
    "    return D_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = MULTI_ROUND_NDMP.to_env()\n",
    "env.reset()\n",
    "print(env.observation_space.n)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.646019389241137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator_1=A_model(env.observation_space.n,env.action_space.n,np.ones(env.observation_space.n))\n",
    "D_real=ReplayMemory(capacity=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_layer,output_layer):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_layer, output_layer,bias=False)\n",
    "        self.fc2=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        x=self.fc1(input_)\n",
    "        y=self.fc2(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self,observation_space,action_space,gamma=0.5,learning_rate=1e-3,horizon_len=20,k=10,fraction_of_real=0.5,batch_size=200):\n",
    "\n",
    "        self.model = Network(observation_space.n,action_space.n)\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "        self.horizon_len=horizon_len # Assuming we already know the horizon length\n",
    "        self.env_model =A_model(observation_space.n,action_space.n,np.ones(observation_space.n))\n",
    "        self.env_models =[] \n",
    "        for i in range(10):\n",
    "            self.env_models.append(A_model(observation_space.n,action_space.n,(0.1*i)+np.ones(observation_space.n)))\n",
    "        for i in self.env_models:\n",
    "            i.terminal_state=observation_space.n-1\n",
    "            i.configure()\n",
    "        \n",
    "        self.env_model.terminal_state=observation_space.n-1\n",
    "        self.env_model.configure()\n",
    "        \n",
    "        self.Is_ensemble=True\n",
    "        self.D_fake=ReplayMemory(capacity=100000)\n",
    "        self.fraction_of_real=fraction_of_real\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        init_weights(self.model)\n",
    "        for i in self.env_models:\n",
    "            i.reset()\n",
    "        self.env_model.reset()\n",
    "        self.D_fake.flush_all()\n",
    "\n",
    "    def action(self, state):\n",
    "        \n",
    "        probs = self.model(Variable(state))\n",
    "        action = probs.multinomial(1).data\n",
    "        prob = probs[:, action[0,0]].view(1, -1)\n",
    "        log_prob = prob.log()\n",
    "\n",
    "        return(action, log_prob)\n",
    "\n",
    "    def index_to_onehot(self,id):\n",
    "        id=torch.tensor(id)\n",
    "        s_t = torch.nn.functional.one_hot(id,num_classes=len(self.env_model.states))\n",
    "        s_t=s_t.type(torch.FloatTensor)\n",
    "        \n",
    "        return s_t\n",
    "\n",
    "    def onehot_to_index(self,x):\n",
    "        id= torch.argmax(x, dim=1)\n",
    "        return id\n",
    "    \n",
    "    def update_D_fake(self,start_state=None,num_of_epi=20):\n",
    "        \n",
    "        if self.Is_ensemble:\n",
    "            env_model_id=np.random.choice(len(self.env_models))\n",
    "            \n",
    "            result=[]\n",
    "            trajs=[]\n",
    "\n",
    "            for traj_id in range(num_of_epi):\n",
    "                \n",
    "                # Start state initialisation\n",
    "                \n",
    "                if start_state is None:\n",
    "                    self.env_models[env_model_id].set_start_state()\n",
    "                    s_t=self.env_models[env_model_id].curr_state\n",
    "                else:\n",
    "                    s_t=start_state\n",
    "                    self.env_models[env_model_id].curr_state=s_t\n",
    "                \n",
    "                # defining empty lists\n",
    "                \n",
    "                states=[]\n",
    "                log_probs=[]\n",
    "                rewards=[]\n",
    "                actions=[]\n",
    "                nstates=[]\n",
    "                \n",
    "                for t in range(4):\n",
    "                    s_t_rep2=self.index_to_onehot(s_t)\n",
    "                    s_t_rep2=torch.unsqueeze(s_t_rep2,0)\n",
    "                    s_t_rep2=s_t_rep2.type(torch.FloatTensor)\n",
    "                    a_t, log_prob = self.action(s_t_rep2)\n",
    "                    ns_t, r_t, done, _ = self.env_models[env_model_id].step(a_t)\n",
    "\n",
    "                    states.append(s_t)\n",
    "                    actions.append(a_t)\n",
    "                    log_probs.append(log_prob)\n",
    "                    rewards.append(r_t)\n",
    "                    nstates.append(ns_t)\n",
    "                    s_t=ns_t\n",
    "                    if done:\n",
    "                        break\n",
    "                self.D_fake.push(states, actions, rewards,nstates, log_probs)      \n",
    "        else:\n",
    "            \n",
    "            result=[]\n",
    "        \n",
    "\n",
    "            trajs=[]\n",
    "\n",
    "            for traj_id in range(num_of_epi):\n",
    "                \n",
    "                # Start state initialisation\n",
    "                \n",
    "                if start_state is None:\n",
    "                    self.env_model.set_start_state()\n",
    "                    s_t=self.env_model.curr_state\n",
    "                else:\n",
    "                    s_t=start_state\n",
    "                    self.env_model.curr_state=s_t\n",
    "                \n",
    "                # defining empty lists\n",
    "                \n",
    "                states=[]\n",
    "                log_probs=[]\n",
    "                rewards=[]\n",
    "                actions=[]\n",
    "                nstates=[]\n",
    "                \n",
    "                for t in range(4):\n",
    "                    s_t_rep2=self.index_to_onehot(s_t)\n",
    "                    s_t_rep2=torch.unsqueeze(s_t_rep2,0)\n",
    "                    s_t_rep2=s_t_rep2.type(torch.FloatTensor)\n",
    "                    a_t, log_prob = self.action(s_t_rep2)\n",
    "                    ns_t, r_t, done, _ = self.env_model.step(a_t)\n",
    "\n",
    "                    states.append(s_t)\n",
    "                    actions.append(a_t)\n",
    "                    log_probs.append(log_prob)\n",
    "                    rewards.append(r_t)\n",
    "                    nstates.append(ns_t)\n",
    "                    s_t=ns_t\n",
    "                    if done:\n",
    "                        break\n",
    "                self.D_fake.push(states, actions, rewards,nstates, log_probs)      \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        return \n",
    "       \n",
    "    def cvt_axis(self,trajs):\n",
    "        t_states = []\n",
    "        t_actions = []\n",
    "        t_rewards = []\n",
    "        t_nstates = []\n",
    "        t_log_probs = []\n",
    "\n",
    "        for traj in trajs:\n",
    "            t_states.append(traj[0])\n",
    "            t_actions.append(traj[1])\n",
    "            t_rewards.append(traj[2])\n",
    "            t_nstates.append(traj[3])\n",
    "            t_log_probs.append(traj[4])\n",
    "\n",
    "        return (t_states, t_actions, t_rewards,t_states,t_log_probs)\n",
    "    \n",
    "    def reward_to_value(self,t_rewards, gamma):\n",
    "\n",
    "        t_Rs = []\n",
    "\n",
    "        for rewards in t_rewards:\n",
    "            Rs = []\n",
    "            R = torch.zeros(1, 1)\n",
    "\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                R = gamma * R + rewards[i]\n",
    "                Rs.insert(0, R)\n",
    "            t_Rs.append(Rs)\n",
    "            \n",
    "        return(t_Rs)\n",
    "\n",
    "    def cal_log_prob(self, state, action):\n",
    "        # state=torch.tensor([state])\n",
    "        state=torch.unsqueeze(state,0)\n",
    "        probs = self.model(Variable(state))\n",
    "        prob = probs[:, action[0,0]].view(1, -1)\n",
    "        log_prob = prob.log()\n",
    "\n",
    "        return(log_prob)\n",
    "    \n",
    "    def MBPO_train_1(self,D_real,mult_fcator=None):\n",
    "        \n",
    "        # Given D_real,and a multiplicative factor,will generate fake_data \n",
    "        # ST :len(D_fake)=multipl_factor*len(D_real)\n",
    "        \n",
    "        self.env_model.reset()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.init_env_model()\n",
    "        \n",
    "        \n",
    "        multiple_factor = (1-self.fraction_of_real)/self.fraction_of_real\n",
    "        if mult_fcator is not None:\n",
    "            multiple_factor=mult_fcator\n",
    "        self.update_D_fake(int(multiple_factor*D_real.position))\n",
    "        data_list=self.D_fake.buffer\n",
    "        \n",
    "        \n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "      \n",
    "    def MBPO_train_2(self,D_real,fraction_of_real=None,mult_factor=1):\n",
    "        \n",
    "        # Given D_real,and the fraction of real to fake trajs,then train the policy on data comprising D_fake and D_real\n",
    "        # ST real_ratio  follows the value given\n",
    "        \n",
    "        for i in range(len(self.env_models)):\n",
    "            self.env_models[i].set_start_state()\n",
    "            self.env_models[i].update_param_given_epi(D_real)\n",
    "            self.env_models[i].set_start_state()\n",
    "            \n",
    "        self.env_model.set_start_state()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.env_model.set_start_state()\n",
    "        \n",
    "        self.D_fake.flush_all()\n",
    "        # NOTE : Here I'm completely flushing out past data and refill again \n",
    "        \n",
    "        self.update_D_fake(None,int(mult_factor*D_real.position))\n",
    "        \n",
    "        frc_of_real=self.fraction_of_real\n",
    "        if fraction_of_real is not None:\n",
    "            frc_of_real=fraction_of_real\n",
    "        \n",
    "        # batch_size=D_real.position\n",
    "        batch_size=self.D_fake.position\n",
    "        \n",
    "        num_of_real_epi=int(batch_size*frc_of_real)\n",
    "        num_of_fake_epi=batch_size-num_of_real_epi\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(self.D_fake.buffer),size=min([num_of_fake_epi,len(self.D_fake.buffer)]),replace=False)\n",
    "        fake_data_list=[self.D_fake.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(D_real.buffer),size=min([num_of_real_epi,len(D_real.buffer)]),replace=False)\n",
    "        real_data_list=[D_real.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        data_list=real_data_list+fake_data_list\n",
    "        \n",
    "        # print(len(real_data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        return\n",
    "\n",
    "    def train_(self, D_real):\n",
    "        \n",
    "        # Pure policy gradient\n",
    "        \n",
    "        data_list=D_real.buffer\n",
    "        # print(len(data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def mod_MBPO_train_2(self,D_real,fraction_of_real=None,mult_factor=1):\n",
    "        \n",
    "        # Given D_real,and the fraction of real to fake trajs,then train the policy on data comprising D_fake and D_real\n",
    "        # ST real_ratio  follows the value given\n",
    "        \n",
    "\n",
    "        for i in range(len(self.env_models)):\n",
    "            self.env_models[i].set_start_state()\n",
    "            self.env_models[i].update_param_given_epi(D_real)\n",
    "            self.env_models[i].set_start_state()\n",
    "            \n",
    "        self.env_model.set_start_state()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.env_model.set_start_state()\n",
    "        # self.D_fake.flush_all()\n",
    "        \n",
    "        # NOTE : Here I'm completely flushing out past data and refill again \n",
    "        # print(self.D_fake.position)\n",
    "        # print(int(mult_factor*D_real.position)-self.D_fake.position)\n",
    "        \n",
    "        self.update_D_fake(None,int(mult_factor*D_real.position)-self.D_fake.position)\n",
    "        \n",
    "        frc_of_real=self.fraction_of_real\n",
    "        if fraction_of_real is not None:\n",
    "            frc_of_real=fraction_of_real\n",
    "        \n",
    "        batch_size=D_real.position\n",
    "        \n",
    "        num_of_real_epi=int(batch_size*frc_of_real)\n",
    "        num_of_fake_epi=batch_size-num_of_real_epi\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(self.D_fake.buffer),size=min([num_of_fake_epi,len(self.D_fake.buffer)]),replace=False)\n",
    "        fake_data_list=[self.D_fake.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(D_real.buffer),size=min([num_of_real_epi,len(D_real.buffer)]),replace=False)\n",
    "        real_data_list=[D_real.buffer[pos] for pos in pos_list]\n",
    "        # print(len(fake_data_list))\n",
    "        data_list=real_data_list+fake_data_list\n",
    "        \n",
    "        # print(len(real_data_list))\n",
    "        # print(len(fake_data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_1=Agent(env.observation_space,env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params=torch.ones(env.action_space.n,env.observation_space.n)/env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  _multi_round_nmdp_simple():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(0) | dsl.reward(10)\n",
    "        start & A_0 > start * 10 | end\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_1 > start * 10 | end * 1 | S_1 * 1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(10)\n",
    "        S_1 & A_1 > start * 5 | end\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate()\n",
    "    \n",
    "def  _multi_round_nmdp_complex_new():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        S_2=dsl.state()\n",
    "        S_3=dsl.state()\n",
    "        S_4=dsl.state()\n",
    "        S_5=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_0 > end * 1 | start * 10\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        start & A_1 > start * 10 | S_1 * 1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 10 | start * 1\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_1 > S_1 * 1 | S_2 * 10\n",
    "        \n",
    "        S_2 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_0 > S_2 * 1 | S_1\n",
    "        S_2 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_1 > S_2 * 1 | S_3\n",
    "        \n",
    "        S_3 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_0 > S_3 * 1 | S_2\n",
    "        S_3 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_1 > S_3 * 1 | S_4 * 10\n",
    "        \n",
    "        S_4 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_0 > S_4 * 1 | S_3\n",
    "        S_4 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_1 > S_4 * 1 | S_5\n",
    "        \n",
    "        S_5 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_5 & A_0 > S_5 * 1 | S_4\n",
    "        S_5 & A_1 > dsl.reward(10) | dsl.reward(0)\n",
    "        S_5 & A_1 > end * 1 | S_1 * 10\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "def  _multi_round_nmdp_complex():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        S_2=dsl.state()\n",
    "        S_3=dsl.state()\n",
    "        S_4=dsl.state()\n",
    "        S_5=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_0 > end * 1 | start\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        start & A_1 > start * 1 | S_1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_1 > S_1 * 1 | S_2\n",
    "        \n",
    "        S_2 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_0 > S_2 * 1 | S_1\n",
    "        S_2 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_1 > S_2 * 1 | S_3\n",
    "        \n",
    "        S_3 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_0 > S_3 * 1 | S_2\n",
    "        S_3 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_1 > S_3 * 1 | S_4\n",
    "        \n",
    "        S_4 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_0 > S_4 * 1 | S_3\n",
    "        S_4 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_1 > S_4 * 1 | S_5\n",
    "        \n",
    "        S_5 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_5 & A_0 > S_5 * 1 | S_4\n",
    "        S_5 & A_1 > dsl.reward(10) | dsl.reward(0)\n",
    "        S_5 & A_1 > end * 1 | S_1\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def  _multi_round_nmdp_complex_v1():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        # S_0=dsl.state()\n",
    "        # S_1=dsl.state()\n",
    "        # S_2=dsl.state()\n",
    "        # S_3=dsl.state()\n",
    "        # S_4=dsl.state()\n",
    "        # S_5=dsl.state()\n",
    "        \n",
    "        state_list=[]\n",
    "        num_of_states_with_term=100\n",
    "        for i in range(num_of_states_with_term):\n",
    "            if i!=num_of_states_with_term-1:\n",
    "                state_list.append(dsl.state())\n",
    "            else:\n",
    "                state_list.append(dsl.terminal_state())\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        state_list[0] & A_0 > dsl.reward(-1) \n",
    "        state_list[0] & A_0 > state_list[0]\n",
    "        state_list[0] & A_1 > dsl.reward(-1) | dsl.reward(0)\n",
    "        state_list[0] & A_1 > state_list[0] * 1 | state_list[1] * 5\n",
    "        \n",
    "        for  i in range(1,num_of_states_with_term-1):\n",
    "            state_list[i] & A_0 > dsl.reward(-1) \n",
    "            state_list[i] & A_0 > state_list[i-1] * 1 \n",
    "            if i+1 != num_of_states_with_term-1:\n",
    "                state_list[i] & A_1 > dsl.reward(0) | dsl.reward(0.1)\n",
    "                state_list[i] & A_1 > state_list[i] * 1 | state_list[i+1] * 5 \n",
    "            else:\n",
    "                state_list[i] & A_1 > dsl.reward(0) | dsl.reward(20)\n",
    "                state_list[i] & A_1 > state_list[i] * 1 | state_list[i+1] * 1 \n",
    "        \n",
    "        \n",
    "        dsl.discount(1)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "\n",
    "\n",
    "MULTI_ROUND_NDMP = _multi_round_nmdp_complex_v1()\n",
    "\n",
    "\n",
    "\n",
    "solver = lp.LinearProgramming(MULTI_ROUND_NDMP)\n",
    "# print(solver.compute_q_table(max_iterations=10000, all_close=functools.partial(np.allclose, rtol=1e-10, atol=1e-10)))\n",
    "env = MULTI_ROUND_NDMP.to_env()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()\n",
    "\n",
    "def expect_num_steps(policy_param,num_of_epochs,Agent):\n",
    "    \n",
    "    A1=Agent\n",
    "    # for param in A1.model.fc1.parameters():\n",
    "    #     param.data = nn.parameter.Parameter(policy_param)\n",
    "    \n",
    "    horizon_len=200\n",
    "    \n",
    "    env.reset()\n",
    "    # print(list(A_1.model.fc1.parameters()))\n",
    "    \n",
    "    s_t_index=env._state.index\n",
    "\n",
    "    trajs=[]\n",
    "    # D_real.flush_all()\n",
    "\n",
    "    result=0\n",
    "\n",
    "    b_count=[]\n",
    "    rewards_sum=[]\n",
    "    for traj_id in range(num_of_epochs):\n",
    "        env.reset()\n",
    "        \n",
    "        # display_env()\n",
    "        s_t_index=env._state.index\n",
    "        # print(s_t_index)\n",
    "        states=[]\n",
    "        log_probs=[]\n",
    "        rewards=[]\n",
    "        actions=[]\n",
    "        nstates=[]\n",
    "        \n",
    "        for t in range(horizon_len):\n",
    "            \n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            a_t, log_prob = A1.action(s_t)\n",
    "            ns_t_index, r_t, done, _ = env.step(a_t.numpy()[0][0])\n",
    "            \n",
    "            states.append(s_t_index)\n",
    "            actions.append(a_t)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(r_t)\n",
    "            nstates.append(ns_t_index)\n",
    "            s_t_index=ns_t_index\n",
    "            if done:\n",
    "                break\n",
    "            b_count.append(len(rewards)) \n",
    "        rewards_sum.append(sum(rewards))  \n",
    "        D_real.push(states, actions, rewards,nstates, log_probs)\n",
    "    \n",
    "    return sum(rewards_sum)/len(rewards_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [01:37<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "[Parameter containing:\n",
      "tensor([[ 1.1999e+00, -3.8566e-01,  5.0641e-01,  2.2392e-01,  1.5965e-01,\n",
      "         -8.3171e-02,  6.5109e-01,  3.1492e-02,  5.3914e-02,  8.1914e-01,\n",
      "         -1.9797e-02,  2.6492e-01,  9.3873e-02,  1.4261e-01,  4.1442e-01,\n",
      "         -5.5665e-02,  9.9882e-01,  3.9015e-01,  1.4157e-02, -9.1027e-02,\n",
      "          5.7501e-01, -6.3272e-02, -4.2834e-02, -1.5504e-02, -6.5965e-02,\n",
      "         -1.1590e-01, -6.9952e-02,  1.9906e-01,  4.3005e-01, -1.6865e-01,\n",
      "         -2.0328e-01, -1.0101e-01,  8.8170e-02,  2.4091e-01,  7.6809e-01,\n",
      "         -2.0598e-01,  5.4265e-02,  3.8811e-03,  1.3902e-01, -3.8630e-02,\n",
      "          2.3329e-01,  3.7541e-01,  1.1464e-01,  4.1757e-01,  1.5382e-01,\n",
      "         -1.3782e-01, -1.8846e-01,  8.3168e-02, -2.4055e-02,  4.5527e-01,\n",
      "          7.1759e-02,  7.5474e-02, -4.9630e-04,  1.5367e-01,  9.0013e-01,\n",
      "         -4.0527e-01,  4.0235e-01,  5.4360e-01,  3.9424e-03, -1.7616e-01,\n",
      "         -4.0765e-01,  4.8565e-01,  4.3389e-01,  7.8748e-01,  2.5841e-01,\n",
      "          2.5018e-01,  1.0451e+00,  3.4907e-01,  4.5753e-01,  9.8865e-01,\n",
      "          4.9942e-01,  3.2170e-01, -3.9009e-01, -1.0365e-01,  1.6413e-01,\n",
      "          4.8398e-01,  4.1318e-02,  6.4001e-01,  2.3466e-01, -1.3633e-01,\n",
      "          2.0419e-01, -3.0269e-02,  5.1143e-01, -7.4879e-02,  5.3976e-01,\n",
      "          4.3912e-02,  2.7145e-01, -2.5974e-01,  2.6735e-01,  4.0210e-03,\n",
      "          1.0838e-01,  1.4226e-01,  9.6005e-01,  5.2638e-01,  3.6624e-01,\n",
      "          1.3089e-01,  3.4504e-01,  2.0322e-01,  8.2374e-01,  5.0000e-01],\n",
      "        [-1.9993e-01,  1.3857e+00,  4.9359e-01,  7.7608e-01,  8.4035e-01,\n",
      "          1.0832e+00,  3.4891e-01,  9.6851e-01,  9.4609e-01,  1.8086e-01,\n",
      "          1.0198e+00,  7.3508e-01,  9.0613e-01,  8.5739e-01,  5.8557e-01,\n",
      "          1.0557e+00,  1.1818e-03,  6.0985e-01,  9.8584e-01,  1.0910e+00,\n",
      "          4.2499e-01,  1.0633e+00,  1.0428e+00,  1.0155e+00,  1.0660e+00,\n",
      "          1.1159e+00,  1.0700e+00,  8.0094e-01,  5.6995e-01,  1.1687e+00,\n",
      "          1.2033e+00,  1.1010e+00,  9.1183e-01,  7.5909e-01,  2.3191e-01,\n",
      "          1.2060e+00,  9.4574e-01,  9.9612e-01,  8.6098e-01,  1.0386e+00,\n",
      "          7.6671e-01,  6.2459e-01,  8.8536e-01,  5.8243e-01,  8.4618e-01,\n",
      "          1.1378e+00,  1.1885e+00,  9.1683e-01,  1.0241e+00,  5.4473e-01,\n",
      "          9.2824e-01,  9.2453e-01,  1.0005e+00,  8.4633e-01,  9.9871e-02,\n",
      "          1.4053e+00,  5.9765e-01,  4.5640e-01,  9.9606e-01,  1.1762e+00,\n",
      "          1.4076e+00,  5.1435e-01,  5.6611e-01,  2.1252e-01,  7.4159e-01,\n",
      "          7.4982e-01, -4.5117e-02,  6.5093e-01,  5.4247e-01,  1.1347e-02,\n",
      "          5.0058e-01,  6.7830e-01,  1.3901e+00,  1.1036e+00,  8.3587e-01,\n",
      "          5.1602e-01,  9.5868e-01,  3.5999e-01,  7.6534e-01,  1.1363e+00,\n",
      "          7.9581e-01,  1.0303e+00,  4.8857e-01,  1.0749e+00,  4.6024e-01,\n",
      "          9.5609e-01,  7.2855e-01,  1.2597e+00,  7.3265e-01,  9.9598e-01,\n",
      "          8.9162e-01,  8.5774e-01,  3.9953e-02,  4.7362e-01,  6.3376e-01,\n",
      "          8.6911e-01,  6.5496e-01,  7.9678e-01,  1.7627e-01,  5.0000e-01]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "A_1=Agent(env.observation_space,env.action_space)\n",
    "for i in A_1.env_models:\n",
    "    i.set_start_state()\n",
    "A_1.env_model.set_start_state()\n",
    "D_real.flush_all()\n",
    "A_1.Is_ensemble=False\n",
    "read_dictionary = np.load('./experiment_new/reward_dict.npy',allow_pickle='TRUE').item()\n",
    "A_1.env_model.state_action_to_reward=read_dictionary\n",
    "init_params=torch.ones(env.action_space.n,env.observation_space.n)/env.action_space.n\n",
    "# init_params=torch.Tensor([[-1.2000, -1.2000, -1.2000, 1.8000, 1.8000, 1.8000, 0.5000],\n",
    "#         [1.8000, 1.8000, 1.8000, -1.2000, -1.2000, -1.2000, 0.5000]])\n",
    "for param in A_1.model.fc1.parameters():\n",
    "    param.data = nn.parameter.Parameter(init_params)\n",
    "print(\"Before\")\n",
    "# print(list(A_1.model.fc1.parameters()))\n",
    "\n",
    "# print(A_1.env_model.state_action_to_reward_dict)\n",
    "quality_list=[]\n",
    "for epoch_id in tqdm(range(600)):\n",
    "    # torch.save(A_1.model.state_dict(), \"./experiment_new/alpha_variation/test_\"+str(epoch_id))\n",
    "    if epoch_id%25==0:\n",
    "        torch.save(A_1.model.state_dict(), \"./experiment_new/alpha_variation_zero_point_two/test_\"+str(epoch_id))\n",
    "    #     print(\"Here\")\n",
    "    #     quality_list.append(expect_num_steps(init_params.float(),1000,A_1))\n",
    "    D_real=update_D_real(D_real,env,A_1,1)\n",
    "    # if epoch_id%200==0:\n",
    "    #     print(list(A_1.model.fc1.parameters()))\n",
    "    for i in range(1):\n",
    "        A_1.mod_MBPO_train_2(D_real,0,1)\n",
    "        # A_1.train_(D_real)\n",
    "print(\"After\")\n",
    "print(list(A_1.model.fc1.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real=update_D_real(D_real,env,A_1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1.env_model.set_start_state()\n",
    "A_1.env_model.update_param_given_epi(D_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A_1.env_model.state_action_to_reward_dict\n",
    "# np.save('./experiment_new/reward_dict.npy',A_1.env_model.state_action_to_reward_dict)\n",
    "read_dictionary = np.load('./experiment_new/reward_dict.npy',allow_pickle='TRUE').item()\n",
    "A_1.env_model.state_action_to_reward=read_dictionary\n",
    "# A_1=Agent(env.observation_space,env.action_space)\n",
    "A_1.env_model.state_action_to_reward=read_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {(State(S0, 0, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S0, 0, False), Action(A1, 1)): [Reward(-1, 1.0),\n",
       "              Reward(0, 1.0)],\n",
       "             (State(S1, 1, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S1, 1, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S2, 2, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S2, 2, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S3, 3, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S3, 3, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S4, 4, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S4, 4, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S5, 5, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S5, 5, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S6, 6, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S6, 6, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S7, 7, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S7, 7, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S8, 8, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S8, 8, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S9, 9, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S9, 9, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S10, 10, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S10, 10, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S11, 11, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S11, 11, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S12, 12, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S12, 12, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S13, 13, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S13, 13, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S14, 14, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S14, 14, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S15, 15, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S15, 15, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S16, 16, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S16, 16, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S17, 17, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S17, 17, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S18, 18, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S18, 18, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S19, 19, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S19, 19, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S20, 20, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S20, 20, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S21, 21, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S21, 21, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S22, 22, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S22, 22, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S23, 23, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S23, 23, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S24, 24, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S24, 24, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S25, 25, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S25, 25, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S26, 26, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S26, 26, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S27, 27, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S27, 27, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S28, 28, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S28, 28, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S29, 29, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S29, 29, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S30, 30, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S30, 30, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S31, 31, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S31, 31, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S32, 32, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S32, 32, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S33, 33, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S33, 33, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S34, 34, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S34, 34, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S35, 35, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S35, 35, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S36, 36, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S36, 36, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S37, 37, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S37, 37, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S38, 38, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S38, 38, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S39, 39, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S39, 39, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S40, 40, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S40, 40, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S41, 41, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S41, 41, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S42, 42, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S42, 42, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S43, 43, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S43, 43, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S44, 44, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S44, 44, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S45, 45, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S45, 45, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S46, 46, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S46, 46, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S47, 47, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S47, 47, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S48, 48, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S48, 48, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S49, 49, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S49, 49, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S50, 50, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S50, 50, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S51, 51, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S51, 51, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S52, 52, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S52, 52, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S53, 53, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S53, 53, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S54, 54, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S54, 54, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S55, 55, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S55, 55, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S56, 56, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S56, 56, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S57, 57, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S57, 57, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S58, 58, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S58, 58, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S59, 59, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S59, 59, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S60, 60, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S60, 60, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S61, 61, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S61, 61, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S62, 62, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S62, 62, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S63, 63, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S63, 63, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S64, 64, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S64, 64, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S65, 65, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S65, 65, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S66, 66, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S66, 66, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S67, 67, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S67, 67, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S68, 68, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S68, 68, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S69, 69, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S69, 69, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S70, 70, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S70, 70, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S71, 71, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S71, 71, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S72, 72, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S72, 72, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S73, 73, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S73, 73, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S74, 74, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S74, 74, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S75, 75, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S75, 75, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S76, 76, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S76, 76, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S77, 77, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S77, 77, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S78, 78, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S78, 78, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S79, 79, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S79, 79, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S80, 80, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S80, 80, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S81, 81, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S81, 81, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S82, 82, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S82, 82, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S83, 83, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S83, 83, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S84, 84, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S84, 84, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S85, 85, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S85, 85, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S86, 86, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S86, 86, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S87, 87, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S87, 87, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S88, 88, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S88, 88, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S89, 89, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S89, 89, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S90, 90, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S90, 90, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S91, 91, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S91, 91, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S92, 92, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S92, 92, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S93, 93, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S93, 93, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S94, 94, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S94, 94, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S95, 95, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S95, 95, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S96, 96, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S96, 96, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S97, 97, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S97, 97, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(0.1, 1.0)],\n",
       "             (State(S98, 98, False), Action(A0, 0)): [Reward(-1, 1.0)],\n",
       "             (State(S98, 98, False), Action(A1, 1)): [Reward(0, 1.0),\n",
       "              Reward(20, 1.0)],\n",
       "             (State(T99, 99, True), Action(A0, 0)): [],\n",
       "             (State(T99, 99, True), Action(A1, 1)): [],\n",
       "             State(S0, 0, False): [],\n",
       "             State(S2, 2, False): [],\n",
       "             (State(S2, 2, False), 0): [],\n",
       "             (State(S4, 4, False), 0): []})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env._state=env.mdp.states[4]\n",
    "# env.mdp.reward_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._state=env.mdp.states[0]\n",
    "curr_state_ind=env._state.index\n",
    "action_space=env.action_space.n\n",
    "for act in action_space:\n",
    "    for i in range(10):\n",
    "        nxt_state_ind,reward,is_done,_=env.step(act)\n",
    "        A_1.env_model.state_action_to_reward_dict[(curr_state_ind,nxt_state_ind)].update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-88.8365000000001, -90.04690000000005, -87.96899999999998, -90.38199999999999, -89.36479999999995, -88.57500000000003, -88.85429999999991, -88.10310000000004, -86.48230000000008, -86.22360000000009, -85.09750000000001, -85.16030000000015, -82.90059999999997, -80.316, -77.91430000000001, -76.46330000000002, -74.27930000000005, -72.40670000000013, -69.90549999999996, -66.43840000000004, -66.33949999999993, -63.380199999999945, -63.12570000000007, -60.39360000000001]\n"
     ]
    }
   ],
   "source": [
    "def path_handler_fun(path):\n",
    "    A_1.model.load_state_dict(torch.load(path))\n",
    "    return expect_num_steps(init_params.float(),1000,A_1)\n",
    "quality_list=[]\n",
    "for epoch_id in range(600):\n",
    "    if epoch_id%25==0:\n",
    "        path= \"./experiment_new/alpha_variation_zero_point_two/test_\"+str(epoch_id)\n",
    "        quality_list.append(path_handler_fun(path))\n",
    "print(quality_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_list=np.array(quality_list)\n",
    "np.savetxt(\"./experiment_new/alpha_variation_zero_point_two/quality_list_0_point_2.csv\", quality_list, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# Y=list(range(5))\n",
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation_zero_point_zero/quality_list_0_point_0.csv\", delimiter=',')\n",
    "plt.plot(my_data,c=\"black\",label=\"0% real\")\n",
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation_zero_point_two/quality_list_0_point_2.csv\", delimiter=',')\n",
    "plt.plot(my_data,c=\"red\",label=\"20% real\")\n",
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation_zero_point_four/quality_list_0_point_4.csv\", delimiter=',')\n",
    "plt.plot(my_data,c=\"green\",label=\"40% real\")\n",
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation_zero_point_six/quality_list_0_point_6.csv\", delimiter=',')\n",
    "plt.plot(my_data,c=\"orange\",label=\"60% real\")\n",
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation_zero_point_eight/quality_list_0_point_8.csv\", delimiter=',')\n",
    "plt.plot(my_data,c=\"blue\",label=\"80% real\")\n",
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation_one_point_zero/quality_list_1_point_0.csv\", delimiter=',')\n",
    "plt.plot(my_data,c=\"cyan\",label=\"100% real\")\n",
    "plt.xlabel(\"epoch/25\")\n",
    "plt.ylabel(\"value function \")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {0: 46.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 51.0}, (0, 1): {0: 30.0, 1: 19.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 0): {0: 66.0, 1: 61.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 1): {0: 1.0, 1: 44.0, 2: 58.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 0): {0: 1.0, 1: 73.0, 2: 84.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 1): {0: 1.0, 1: 1.0, 2: 54.0, 3: 61.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 0): {0: 1.0, 1: 1.0, 2: 64.0, 3: 82.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 63.0, 4: 73.0, 5: 1.0, 6: 1.0}, (4, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 65.0, 4: 57.0, 5: 1.0, 6: 1.0}, (4, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 49.0, 5: 53.0, 6: 1.0}, (5, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 30.0, 5: 29.0, 6: 1.0}, (5, 1): {0: 1.0, 1: 24.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 18.0}, (6, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (6, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A_1.env_model.terminal_state=env.observation_space.n-1\n",
    "print(A_1.env_model.state_action_to_state_dict)\n",
    "# print(A_1.env_model.state_action_to_reward_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expect_num_steps(policy_param,num_of_epochs,Agent):\n",
    "    \n",
    "    A1=Agent\n",
    "    # for param in A1.model.fc1.parameters():\n",
    "    #     param.data = nn.parameter.Parameter(policy_param)\n",
    "    \n",
    "    horizon_len=200\n",
    "    \n",
    "    env.reset()\n",
    "    # print(list(A_1.model.fc1.parameters()))\n",
    "    \n",
    "    s_t_index=env._state.index\n",
    "\n",
    "    trajs=[]\n",
    "    # D_real.flush_all()\n",
    "\n",
    "    result=0\n",
    "\n",
    "    b_count=[]\n",
    "    rewards_sum=[]\n",
    "    for traj_id in range(num_of_epochs):\n",
    "        env.reset()\n",
    "        # display_env()\n",
    "        s_t_index=env._state.index\n",
    "        \n",
    "        states=[]\n",
    "        log_probs=[]\n",
    "        rewards=[]\n",
    "        actions=[]\n",
    "        nstates=[]\n",
    "        \n",
    "        for t in range(horizon_len):\n",
    "            \n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            a_t, log_prob = A1.action(s_t)\n",
    "            ns_t_index, r_t, done, _ = env.step(a_t.numpy()[0][0])\n",
    "            \n",
    "            states.append(s_t_index)\n",
    "            actions.append(a_t)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(r_t)\n",
    "            nstates.append(ns_t_index)\n",
    "            s_t_index=ns_t_index\n",
    "            if done:\n",
    "                break\n",
    "            b_count.append(len(rewards)) \n",
    "        rewards_sum.append(sum(rewards))  \n",
    "        D_real.push(states, actions, rewards,nstates, log_probs)\n",
    "    \n",
    "    return sum(rewards_sum)/len(rewards_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params=torch.Tensor([[ 0.6825,  0.8863, -1.2302, -1.1798, -1.1560, -1.0646, -1.1338, -1.0857,\n",
    "         -1.0740,  1.0120, -1.1996, -1.1750, -1.2342, -0.2169, -1.2770, -1.2170,\n",
    "         -1.1092, -1.1832, -1.1482, -0.8633,  1.4509, -0.9253, -0.9925, -0.7671,\n",
    "          1.2539, -1.0963, -1.0425, -0.9551, -1.1699, -0.9062, -1.1976, -1.0136,\n",
    "         -0.9320, -1.0067, -1.0133, -1.0154, -1.1422, -1.1834, -1.1957, -1.1596,\n",
    "         -1.2002, -1.1642, -1.1950, -1.1949, -1.1934, -1.2006, -0.1996, -0.8419,\n",
    "         -1.1900, -1.2040, -1.2178, -1.0663, -1.2266, -1.2790, -1.2411, -1.2221,\n",
    "         -1.2150, -1.2267, -1.2698, -1.1747, -1.1789,  0.0350, -1.1914, -1.1846,\n",
    "         -1.1283, -1.2682, -1.1442, -1.2541, -1.2471, -1.2594, -1.2120, -1.2714,\n",
    "         -1.2630, -1.1423, -1.1326, -1.1313, -1.1311, -1.1213, -0.2080, -1.0810,\n",
    "         -0.3903, -1.1010, -1.0920, -1.1611, -1.1629, -1.1649, -1.2086,  0.6375,\n",
    "         -1.2256, -1.2167, -0.6320, -1.1864, -1.2075, -1.1824, -1.1949, -1.2287,\n",
    "         -1.0093, -0.9339, -0.9244,  0.5000],\n",
    "        [ 0.3175,  0.1137,  2.2302,  2.1798,  2.1560,  2.0646,  2.1338,  2.0858,\n",
    "          2.0740, -0.0120,  2.1996,  2.1750,  2.2342,  1.2169,  2.2770,  2.2170,\n",
    "          2.1092,  2.1832,  2.1482,  1.8633, -0.4509,  1.9253,  1.9925,  1.7671,\n",
    "         -0.2539,  2.0963,  2.0425,  1.9551,  2.1699,  1.9062,  2.1976,  2.0136,\n",
    "          1.9320,  2.0067,  2.0133,  2.0154,  2.1422,  2.1834,  2.1957,  2.1596,\n",
    "          2.2002,  2.1642,  2.1950,  2.1949,  2.1934,  2.2006,  1.1996,  1.8419,\n",
    "          2.1900,  2.2040,  2.2178,  2.0663,  2.2266,  2.2790,  2.2411,  2.2221,\n",
    "          2.2150,  2.2267,  2.2698,  2.1747,  2.1789,  0.9650,  2.1914,  2.1846,\n",
    "          2.1283,  2.2682,  2.1442,  2.2541,  2.2471,  2.2594,  2.2120,  2.2714,\n",
    "          2.2630,  2.1423,  2.1326,  2.1313,  2.1311,  2.1213,  1.2080,  2.0810,\n",
    "          1.3903,  2.1010,  2.0920,  2.1611,  2.1629,  2.1649,  2.2086,  0.3625,\n",
    "          2.2256,  2.2167,  1.6320,  2.1863,  2.2075,  2.1824,  2.1949,  2.2287,\n",
    "          2.0093,  1.9339,  1.9244,  0.5000]])\n",
    "# init_params=torch.ones(env.action_space.n,env.observation_space.n)/env.action_space.n\n",
    "init_para=init_params.numpy()\n",
    "np.savetxt(\"./experiment_new/alpha_variation/dummy_exp.csv\", init_para, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "19\n",
      "80\n",
      "2\n",
      "22\n",
      "62\n",
      "24\n",
      "6\n",
      "93\n",
      "64\n",
      "-86.44000000000003\n"
     ]
    }
   ],
   "source": [
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation/dummy_exp.csv\", delimiter=',')\n",
    "# my_data = np.genfromtxt(\"./experiment/init_param_big.csv\", delimiter=',')\n",
    "# print(my_data)\n",
    "\n",
    "init_params=torch.from_numpy(my_data)\n",
    "rew_with_good=expect_num_steps(init_params.float(),10,A_1)\n",
    "print(rew_with_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {0: 386.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 317.0}, (0, 1): {0: 122.0, 1: 165.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 0): {0: 411.0, 1: 446.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 1): {0: 1.0, 1: 229.0, 2: 218.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 0): {0: 1.0, 1: 206.0, 2: 212.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 1): {0: 1.0, 1: 1.0, 2: 303.0, 3: 315.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 0): {0: 1.0, 1: 1.0, 2: 217.0, 3: 181.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 311.0, 4: 336.0, 5: 1.0, 6: 1.0}, (4, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 146.0, 4: 192.0, 5: 1.0, 6: 1.0}, (4, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 322.0, 5: 356.0, 6: 1.0}, (5, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 90.0, 5: 102.0, 6: 1.0}, (5, 1): {0: 1.0, 1: 175.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 174.0}, (6, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (6, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}}\n"
     ]
    }
   ],
   "source": [
    "print(A_1.env_model.state_action_to_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "{(0, 0): {0: 0, 1: 0.0843690660432731, 2: 0.849836236335819, 3: 0.04269775042997859, 4: 0.6927689805985797, 5: 0.46587405632402956, 6: 10}, (0, 1): {0: 0, 1: 0, 2: 0.6579976854489147, 3: 0.6804551352408054, 4: 0.7221623590673689, 5: 0.3633184088247551, 6: 0.7105673467045505}, (1, 0): {0: 0, 1: 0, 2: 0.6602864723695561, 3: 0.4682956991167345, 4: 0.7539382208410857, 5: 0.8131153643545767, 6: 0.2125786449374153}, (1, 1): {0: 0.3506829104996698, 1: 0, 2: 0, 3: 0.37592047356796876, 4: 0.10817453678234279, 5: 0.709789260139768, 6: 0.30157215691137595}, (2, 0): {0: 0.9298344194564789, 1: 0, 2: 0, 3: 0.7851022144181312, 4: 0.6277917767593851, 5: 0.6000684898476814, 6: 0.3838554528756474}, (2, 1): {0: 0.9338957960775606, 1: 0.8208397716569739, 2: 0, 3: 0, 4: 0.9399701642517263, 5: 0.4573391290634353, 6: 0.03543488339963141}, (3, 0): {0: 0.3860803130577549, 1: 0.031673115214396974, 2: 0, 3: 0, 4: 0.21732761399517841, 5: 0.138098130873191, 6: 0.6636311180979971}, (3, 1): {0: 0.7300320306288464, 1: 0.39193362171466595, 2: 0.9077702861230729, 3: 0, 4: 0, 5: 0.6437299304572983, 6: 0.39433859745819855}, (4, 0): {0: 0.5969588029886236, 1: 0.6998937368043668, 2: 0.9301717692295011, 3: 0, 4: 0, 5: 0.5452251602331716, 6: 0.3034097195880172}, (4, 1): {0: 0.49387473239656376, 1: 0.9112735705891571, 2: 0.8773877517529274, 3: 0.7862639505908811, 4: 0, 5: 0, 6: 0.4074881787940904}, (5, 0): {0: 0.6217142198987423, 1: 0.5725700482174924, 2: 0.18013727933034218, 3: 0.8175302964233415, 4: 0, 5: 0, 6: 0.7242699392460611}, (5, 1): {0: 0.7263452526725953, 1: 0, 2: 0.07434070172402518, 3: 0.4325186550233542, 4: 0.07526707936588373, 5: 0.2843880944627756, 6: 10}, (6, 0): {0: 0.9771615310739961, 1: 0.8535428725020938, 2: 0.5757307076187722, 3: 0.09070808669258923, 4: 0.626597340432684, 5: 0.17769346927439855, 6: 0.8553771996656884}, (6, 1): {0: 0.23950220221019136, 1: 0.7636698416987175, 2: 0.6476562865206991, 3: 0.8773698018071847, 4: 0.2049304251293873, 5: 0.7831247525202525, 6: 0.9292510782205334}}\n"
     ]
    }
   ],
   "source": [
    "print(\"After\")\n",
    "# A_1.env_model.terminal_state=env.observation_space.n-1\n",
    "print(A_1.env_model.state_action_to_reward_dict)\n",
    "# print(A_1.env_model.state_action_to_reward_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real=update_D_real(D_real,env,A_1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "A_1.env_model.update_param_given_epi(D_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_onehot(id):\n",
    "    id=torch.tensor(id)\n",
    "    s_t = torch.nn.functional.one_hot(id,num_classes=7)\n",
    "    s_t=s_t.type(torch.FloatTensor)\n",
    "    \n",
    "    return s_t\n",
    "\n",
    "def onehot_to_index(self,x):\n",
    "    id= torch.argmax(x, dim=1)\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "A_1.MBPO_train_2(D_real,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
