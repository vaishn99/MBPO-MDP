{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "\n",
    "from blackhc.mdp import dsl\n",
    "from blackhc import mdp\n",
    "import time\n",
    "\n",
    "from blackhc.mdp import lp\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    def flush_all(self):\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        return\n",
    "\n",
    "    def push(self, state, action, reward, next_state,policy):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state,policy)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def push_batch(self, batch):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            append_len = min(self.capacity - len(self.buffer), len(batch))\n",
    "            self.buffer.extend([None] * append_len)\n",
    "\n",
    "        if self.position + len(batch) < self.capacity:\n",
    "            self.buffer[self.position : self.position + len(batch)] = batch\n",
    "            self.position += len(batch)\n",
    "        else:\n",
    "            self.buffer[self.position : len(self.buffer)] = batch[:len(self.buffer) - self.position]\n",
    "            self.buffer[:len(batch) - len(self.buffer) + self.position] = batch[len(self.buffer) - self.position:]\n",
    "            self.position = len(batch) - len(self.buffer) + self.position\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        batch = random.sample(self.buffer, int(batch_size))\n",
    "        state, action, reward, next_state,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state,policy\n",
    "\n",
    "    def sample_all_batch(self, batch_size):\n",
    "        idxes = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = list(itemgetter(*idxes)(self.buffer))\n",
    "        state, action, reward, next_state,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state,policy\n",
    "\n",
    "    def return_all(self):\n",
    "        return self.buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption : The reward function is known prior:\n",
    "import itertools\n",
    "\n",
    "class A_model:\n",
    "    def __init__(self,num_of_states,num_of_actions,prior_vec):\n",
    "        self.states = np.arange(num_of_states)\n",
    "        self.actions = np.arange(num_of_actions)\n",
    "        self.terminal_state = None\n",
    "        self.state_action_to_state_dict=dict()\n",
    "        self.state_action_to_reward_dict=dict()\n",
    "        self.prior_vec = prior_vec\n",
    "        \n",
    "        self.curr_state = None\n",
    "        self.horizon_len=20 # this specifies the horizon_len for updating D_real\n",
    "        self.last_seen_len=0 # assuming D_real with infinit capacity\n",
    "        self.k=10 # specifies the horizon len for updating_len for updating D_fake  \n",
    "        \n",
    "    def configure(self):\n",
    "        # Initialisng the transition prob_matrix\n",
    "        key_list=list(itertools.product(self.states, self.actions))\n",
    "        for x in key_list:\n",
    "            self.state_action_to_state_dict.update({x:dict()})\n",
    "            for y in range(len(self.states)):\n",
    "                self.state_action_to_state_dict[x].update({self.states[y]:self.prior_vec[y]})\n",
    "        # init the rewrad matrix\n",
    "        \n",
    "        for x in key_list:\n",
    "            self.state_action_to_reward_dict.update({x:dict()})\n",
    "            for y in range(len(self.states)):\n",
    "                if self.states[y]==self.terminal_state:\n",
    "                    self.state_action_to_reward_dict[x].update({self.states[y]:10})\n",
    "                else:\n",
    "                    self.state_action_to_reward_dict[x].update({self.states[y]:0})\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state_action_to_state_dict=dict()\n",
    "        self.state_action_to_reward_dict=dict()\n",
    "        self.last_seen_len=0\n",
    "    \n",
    "    def update_param_given_epi(self,D_real):\n",
    "        \n",
    "        episodes=D_real.buffer[self.last_seen_len:len(D_real.buffer)]\n",
    "        self.last_seen_len=len(D_real.buffer)\n",
    "\n",
    "        # print(len(episodes))\n",
    "        \n",
    "        \n",
    "        # following SARSA format\n",
    "        for epi_id in range(len(episodes)):\n",
    "    \n",
    "            t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(episodes[epi_id])\n",
    "            i=0 \n",
    "            \n",
    "            while i<len(t_states):\n",
    "                \n",
    "                # updating the list of states\n",
    "                \n",
    "                # tru_tup=self.state_action_to_state_dict[(t_states[i],t_actions[i].item())]\n",
    "\n",
    "\n",
    "                self.state_action_to_state_dict[(t_states[i],t_actions[i].item())][t_nstates[i]]+=1\n",
    "                self.state_action_to_reward_dict[(t_states[i],t_actions[i].item())][t_nstates[i]]=t_rewards[i]\n",
    "                                    \n",
    "                i+=1  \n",
    "                \n",
    "            if self.terminal_state is None and i<self.horizon_len:\n",
    "                self.terminal_state=t_nstates[i-1]\n",
    "                self.state_list.append(t_nstates[i-1])\n",
    "        return\n",
    "\n",
    "    def cvt_axis(self,traj):\n",
    "        \n",
    "        t_states =[]\n",
    "        t_actions =[]\n",
    "        t_nstates =[]\n",
    "        t_rewards=[]\n",
    "        t_log_probs=[]\n",
    "        \n",
    "        for i in range(len(traj[0])):\n",
    "            t_states.append(traj[0][i])\n",
    "            t_actions.append(traj[1][i])\n",
    "            t_rewards.append(traj[2][i])\n",
    "            t_nstates.append(traj[3][i])\n",
    "            t_log_probs.append(traj[4][i])\n",
    "\n",
    "        return (t_states, t_actions, t_rewards,t_nstates,t_log_probs) \n",
    "    \n",
    "    def step(self,a_t):\n",
    "        \n",
    "        next_state=0\n",
    "        un_norm_distr=self.state_action_to_state_dict[(self.curr_state,a_t.item())]\n",
    "        norm_factor=sum(list(un_norm_distr.values()))\n",
    "        choices=list(un_norm_distr.keys())\n",
    "        p=[x/norm_factor for x in un_norm_distr.values()]\n",
    "        \n",
    "        next_state_id=np.random.choice(np.arange(len(choices)),p=p)\n",
    "        next_state=choices[next_state_id]\n",
    "        reward=self.state_action_to_reward_dict[(self.curr_state,a_t.item())][next_state]\n",
    "\n",
    "        self.curr_state=next_state\n",
    "        Is_done=False\n",
    "        if self.terminal_state==next_state:\n",
    "            Is_done=True\n",
    "\n",
    "        return next_state,reward,Is_done,None\n",
    "\n",
    "    def set_start_state(self):\n",
    "        self.curr_state=np.random.choice(self.states)\n",
    "        while self.curr_state==self.terminal_state:\n",
    "            self.curr_state=np.random.choice(self.states)\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  _multi_round_nmdp_simple():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(0) | dsl.reward(10)\n",
    "        start & A_0 > start * 10 | end\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_1 > start * 10 | end * 1 | S_1 * 1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(10)\n",
    "        S_1 & A_1 > start * 5 | end\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate()\n",
    "    \n",
    "def  _multi_round_nmdp_complex():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        S_2=dsl.state()\n",
    "        S_3=dsl.state()\n",
    "        S_4=dsl.state()\n",
    "        S_5=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_0 > end * 1 | start\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        start & A_1 > start * 1 | S_1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_1 > S_1 * 1 | S_2\n",
    "        \n",
    "        S_2 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_0 > S_2 * 1 | S_1\n",
    "        S_2 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_1 > S_2 * 1 | S_3\n",
    "        \n",
    "        S_3 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_0 > S_3 * 1 | S_2\n",
    "        S_3 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_1 > S_3 * 1 | S_4\n",
    "        \n",
    "        S_4 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_0 > S_4 * 1 | S_3\n",
    "        S_4 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_1 > S_4 * 1 | S_5\n",
    "        \n",
    "        S_5 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_5 & A_0 > S_5 * 1 | S_4\n",
    "        S_5 & A_1 > dsl.reward(10) | dsl.reward(0)\n",
    "        S_5 & A_1 > end * 1 | S_1\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "MULTI_ROUND_NDMP = _multi_round_nmdp_complex()\n",
    "\n",
    "\n",
    "\n",
    "solver = lp.LinearProgramming(MULTI_ROUND_NDMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# update D_real\n",
    "\n",
    "\n",
    "def update_D_real(D_real,env,Agent,num_of_epochs):\n",
    "    \n",
    "    A1=Agent\n",
    "    \n",
    "    horizon_len=20\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    s_t_index=env._state.index\n",
    "\n",
    "    trajs=[]\n",
    "    # D_real.flush_all()\n",
    "\n",
    "    result=0\n",
    "\n",
    "\n",
    "    for traj_id in range(num_of_epochs):\n",
    "        env.reset()\n",
    "        # display_env()\n",
    "        s_t_index=env._state.index\n",
    "        \n",
    "        states=[]\n",
    "        log_probs=[]\n",
    "        rewards=[]\n",
    "        actions=[]\n",
    "        nstates=[]\n",
    "        \n",
    "        for t in range(horizon_len):\n",
    "            \n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            a_t, log_prob = A1.action(s_t)\n",
    "            ns_t_index, r_t, done, _ = env.step(a_t.numpy()[0][0])\n",
    "            \n",
    "            states.append(s_t_index)\n",
    "            actions.append(a_t)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(r_t)\n",
    "            nstates.append(ns_t_index)\n",
    "            s_t_index=ns_t_index\n",
    "            if done:\n",
    "                break   \n",
    "        D_real.push(states, actions, rewards,nstates, log_probs)\n",
    "    \n",
    "    return D_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = MULTI_ROUND_NDMP.to_env()\n",
    "env.reset()\n",
    "print(env.observation_space.n)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_1=A_model(env.observation_space.n,env.action_space.n,np.ones(env.observation_space.n))\n",
    "D_real=ReplayMemory(capacity=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_layer,output_layer):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_layer, output_layer,bias=False)\n",
    "        self.fc2=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        x=self.fc1(input_)\n",
    "        y=self.fc2(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self,observation_space,action_space,gamma=0.99,learning_rate=1e-3,horizon_len=20,k=10,fraction_of_real=0.5,batch_size=200):\n",
    "\n",
    "        self.model = Network(observation_space.n,action_space.n)\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "        self.horizon_len=horizon_len # Assuming we already know the horizon length\n",
    "        self.env_model =A_model(observation_space.n,action_space.n,np.ones(observation_space.n))\n",
    "        self.env_model.terminal_state=observation_space.n-1\n",
    "        self.env_model.configure()\n",
    "        \n",
    "        self.D_fake=ReplayMemory(capacity=100000)\n",
    "        self.fraction_of_real=fraction_of_real\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        init_weights(self.model)\n",
    "        self.env_model.reset()\n",
    "        self.D_fake.flush_all()\n",
    "\n",
    "    def action(self, state):\n",
    "        \n",
    "        probs = self.model(Variable(state))\n",
    "        action = probs.multinomial(1).data\n",
    "        prob = probs[:, action[0,0]].view(1, -1)\n",
    "        log_prob = prob.log()\n",
    "\n",
    "        return(action, log_prob)\n",
    "\n",
    "    def index_to_onehot(self,id):\n",
    "        id=torch.tensor(id)\n",
    "        s_t = torch.nn.functional.one_hot(id,num_classes=len(self.env_model.states))\n",
    "        s_t=s_t.type(torch.FloatTensor)\n",
    "        \n",
    "        return s_t\n",
    "\n",
    "    def onehot_to_index(self,x):\n",
    "        id= torch.argmax(x, dim=1)\n",
    "        return id\n",
    "    \n",
    "    def update_D_fake(self,start_state=None,num_of_epi=20):\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        result=[]\n",
    "        \n",
    "\n",
    "        trajs=[]\n",
    "\n",
    "        for traj_id in range(num_of_epi):\n",
    "            \n",
    "            # Start state initialisation\n",
    "            \n",
    "            if start_state is None:\n",
    "                self.env_model.set_start_state()\n",
    "                s_t=self.env_model.curr_state\n",
    "            else:\n",
    "                s_t=start_state\n",
    "                self.env_model.curr_state=s_t\n",
    "            \n",
    "            # defining empty lists\n",
    "             \n",
    "            states=[]\n",
    "            log_probs=[]\n",
    "            rewards=[]\n",
    "            actions=[]\n",
    "            nstates=[]\n",
    "            \n",
    "            for t in range(self.env_model.k):\n",
    "                s_t_rep2=self.index_to_onehot(s_t)\n",
    "                s_t_rep2=torch.unsqueeze(s_t_rep2,0)\n",
    "                s_t_rep2=s_t_rep2.type(torch.FloatTensor)\n",
    "                a_t, log_prob = self.action(s_t_rep2)\n",
    "                ns_t, r_t, done, _ = self.env_model.step(a_t)\n",
    "\n",
    "                states.append(s_t)\n",
    "                actions.append(a_t)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(r_t)\n",
    "                nstates.append(ns_t)\n",
    "                s_t=ns_t\n",
    "                if done:\n",
    "                    break\n",
    "            self.D_fake.push(states, actions, rewards,nstates, log_probs)      \n",
    "        return \n",
    "       \n",
    "    def cvt_axis(self,trajs):\n",
    "        t_states = []\n",
    "        t_actions = []\n",
    "        t_rewards = []\n",
    "        t_nstates = []\n",
    "        t_log_probs = []\n",
    "\n",
    "        for traj in trajs:\n",
    "            t_states.append(traj[0])\n",
    "            t_actions.append(traj[1])\n",
    "            t_rewards.append(traj[2])\n",
    "            t_nstates.append(traj[3])\n",
    "            t_log_probs.append(traj[4])\n",
    "\n",
    "        return (t_states, t_actions, t_rewards,t_states,t_log_probs)\n",
    "    \n",
    "    def reward_to_value(self,t_rewards, gamma):\n",
    "\n",
    "        t_Rs = []\n",
    "\n",
    "        for rewards in t_rewards:\n",
    "            Rs = []\n",
    "            R = torch.zeros(1, 1)\n",
    "\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                R = gamma * R + rewards[i]\n",
    "                Rs.insert(0, R)\n",
    "            t_Rs.append(Rs)\n",
    "            \n",
    "        return(t_Rs)\n",
    "\n",
    "    def cal_log_prob(self, state, action):\n",
    "        # state=torch.tensor([state])\n",
    "        state=torch.unsqueeze(state,0)\n",
    "        probs = self.model(Variable(state))\n",
    "        prob = probs[:, action[0,0]].view(1, -1)\n",
    "        log_prob = prob.log()\n",
    "\n",
    "        return(log_prob)\n",
    "    \n",
    "    def MBPO_train_1(self,D_real,mult_fcator=None):\n",
    "        \n",
    "        # Given D_real,and a multiplicative factor,will generate fake_data \n",
    "        # ST :len(D_fake)=multipl_factor*len(D_real)\n",
    "        \n",
    "        self.env_model.reset()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.init_env_model()\n",
    "        \n",
    "        multiple_factor = (1-self.fraction_of_real)/self.fraction_of_real\n",
    "        if mult_fcator is not None:\n",
    "            multiple_factor=mult_fcator\n",
    "        self.update_D_fake(int(multiple_factor*D_real.position))\n",
    "        data_list=self.D_fake.buffer\n",
    "        \n",
    "        \n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "      \n",
    "    def MBPO_train_2(self,D_real,fraction_of_real=None,mult_factor=1):\n",
    "        \n",
    "        # Given D_real,and the fraction of real to fake trajs,then train the policy on data comprising D_fake and D_real\n",
    "        # ST real_ratio  follows the value given\n",
    "        \n",
    "\n",
    "        self.env_model.set_start_state()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.env_model.set_start_state()\n",
    "        self.D_fake.flush_all()\n",
    "        # NOTE : Here I'm completely flushing out past data and refill again \n",
    "        \n",
    "        self.update_D_fake(None,int(mult_factor*D_real.position))\n",
    "        \n",
    "        frc_of_real=self.fraction_of_real\n",
    "        if fraction_of_real is not None:\n",
    "            frc_of_real=fraction_of_real\n",
    "        \n",
    "        batch_size=D_real.position\n",
    "        \n",
    "        num_of_real_epi=int(batch_size*frc_of_real)\n",
    "        num_of_fake_epi=batch_size-num_of_real_epi\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(self.D_fake.buffer),size=min([num_of_fake_epi,len(self.D_fake.buffer)]),replace=False)\n",
    "        fake_data_list=[self.D_fake.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(D_real.buffer),size=min([num_of_real_epi,len(D_real.buffer)]),replace=False)\n",
    "        real_data_list=[D_real.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        data_list=real_data_list+fake_data_list\n",
    "        \n",
    "        # print(len(real_data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        return\n",
    "\n",
    "    def train_(self, D_real):\n",
    "        \n",
    "        # Pure policy gradient\n",
    "        \n",
    "        data_list=D_real.buffer\n",
    "        # print(len(data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def mod_MBPO_train_2(self,D_real,fraction_of_real=None,mult_factor=1):\n",
    "        \n",
    "        # Given D_real,and the fraction of real to fake trajs,then train the policy on data comprising D_fake and D_real\n",
    "        # ST real_ratio  follows the value given\n",
    "        \n",
    "\n",
    "        self.env_model.set_start_state()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.env_model.set_start_state()\n",
    "        # self.D_fake.flush_all()\n",
    "        \n",
    "        # NOTE : Here I'm completely flushing out past data and refill again \n",
    "        # print(self.D_fake.position)\n",
    "        # print(int(mult_factor*D_real.position)-self.D_fake.position)\n",
    "        \n",
    "        self.update_D_fake(None,int(mult_factor*D_real.position)-self.D_fake.position)\n",
    "        \n",
    "        frc_of_real=self.fraction_of_real\n",
    "        if fraction_of_real is not None:\n",
    "            frc_of_real=fraction_of_real\n",
    "        \n",
    "        batch_size=D_real.position\n",
    "        \n",
    "        num_of_real_epi=int(batch_size*frc_of_real)\n",
    "        num_of_fake_epi=batch_size-num_of_real_epi\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(self.D_fake.buffer),size=min([num_of_fake_epi,len(self.D_fake.buffer)]),replace=False)\n",
    "        fake_data_list=[self.D_fake.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(D_real.buffer),size=min([num_of_real_epi,len(D_real.buffer)]),replace=False)\n",
    "        real_data_list=[D_real.buffer[pos] for pos in pos_list]\n",
    "        # print(len(fake_data_list))\n",
    "        data_list=real_data_list+fake_data_list\n",
    "        \n",
    "        # print(len(real_data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(self.index_to_onehot(states[t]), actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        return\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_1=Agent(env.observation_space,env.action_space)\n",
    "A_1.env_model.set_start_state()\n",
    "D_real.flush_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params=torch.ones(env.action_space.n,env.observation_space.n)/env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.16666667 4.20138889]\n",
      " [0.76388889 0.25462963]\n",
      " [0.25462963 0.21219136]\n",
      " [0.21219136 0.5941358 ]\n",
      " [0.5941358  1.78240741]\n",
      " [1.78240741 5.34722222]\n",
      " [0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  _multi_round_nmdp_simple():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(0) | dsl.reward(10)\n",
    "        start & A_0 > start * 10 | end\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_1 > start * 10 | end * 1 | S_1 * 1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(10)\n",
    "        S_1 & A_1 > start * 5 | end\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate()\n",
    "    \n",
    "def  _multi_round_nmdp_complex():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        S_2=dsl.state()\n",
    "        S_3=dsl.state()\n",
    "        S_4=dsl.state()\n",
    "        S_5=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_0 > end * 1 | start * 10\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        start & A_1 > start * 10 | S_1 * 1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 10 | start * 1\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_1 > S_1 * 1 | S_2\n",
    "        \n",
    "        S_2 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_0 > S_2 * 1 | S_1\n",
    "        S_2 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_1 > S_2 * 1 | S_3\n",
    "        \n",
    "        S_3 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_0 > S_3 * 1 | S_2\n",
    "        S_3 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_1 > S_3 * 1 | S_4\n",
    "        \n",
    "        S_4 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_0 > S_4 * 1 | S_3\n",
    "        S_4 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_1 > S_4 * 1 | S_5\n",
    "        \n",
    "        S_5 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_5 & A_0 > S_5 * 1 | S_4\n",
    "        S_5 & A_1 > dsl.reward(10) | dsl.reward(0)\n",
    "        S_5 & A_1 > end * 1 | S_1 * 10\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "MULTI_ROUND_NDMP = _multi_round_nmdp_complex()\n",
    "\n",
    "\n",
    "\n",
    "solver = lp.LinearProgramming(MULTI_ROUND_NDMP)\n",
    "print(solver.compute_q_table(max_iterations=10000, all_close=functools.partial(np.allclose, rtol=1e-10, atol=1e-10)))\n",
    "env = MULTI_ROUND_NDMP.to_env()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "[Parameter containing:\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/600 [00:00<00:04, 119.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 24/600 [00:00<00:07, 74.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.4859, 0.4812, 0.4908, 0.5173, 0.4844, 0.4950, 0.5000],\n",
      "        [0.5141, 0.5188, 0.5092, 0.4827, 0.5156, 0.5050, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 53/600 [00:01<00:20, 26.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.4923, 0.4766, 0.4976, 0.5373, 0.4730, 0.4746, 0.5000],\n",
      "        [0.5077, 0.5234, 0.5024, 0.4627, 0.5270, 0.5254, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 76/600 [00:03<00:38, 13.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5021, 0.4777, 0.5074, 0.5472, 0.4614, 0.4574, 0.5000],\n",
      "        [0.4979, 0.5223, 0.4926, 0.4528, 0.5386, 0.5426, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 101/600 [00:05<00:55,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5171, 0.4810, 0.5114, 0.5492, 0.4493, 0.4559, 0.5000],\n",
      "        [0.4829, 0.5190, 0.4886, 0.4508, 0.5507, 0.5441, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 126/600 [00:09<01:04,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5363, 0.4855, 0.5115, 0.5509, 0.4392, 0.4625, 0.5000],\n",
      "        [0.4637, 0.5145, 0.4885, 0.4491, 0.5608, 0.5375, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 151/600 [00:12<01:14,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5560, 0.4911, 0.5137, 0.5561, 0.4285, 0.4691, 0.5000],\n",
      "        [0.4440, 0.5089, 0.4863, 0.4439, 0.5715, 0.5309, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 176/600 [00:17<01:23,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5747, 0.4969, 0.5169, 0.5620, 0.4176, 0.4687, 0.5000],\n",
      "        [0.4253, 0.5031, 0.4831, 0.4380, 0.5824, 0.5313, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 200/600 [00:22<01:29,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5913, 0.5027, 0.5210, 0.5660, 0.4070, 0.4613, 0.5000],\n",
      "        [0.4087, 0.4973, 0.4790, 0.4340, 0.5930, 0.5387, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 225/600 [00:28<01:35,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.6118, 0.5117, 0.5263, 0.5693, 0.3964, 0.4490, 0.5000],\n",
      "        [0.3882, 0.4883, 0.4737, 0.4307, 0.6036, 0.5510, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 250/600 [00:35<01:39,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.6345, 0.5240, 0.5329, 0.5731, 0.3864, 0.4348, 0.5000],\n",
      "        [0.3655, 0.4760, 0.4671, 0.4269, 0.6136, 0.5652, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 275/600 [00:43<01:41,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.6582, 0.5389, 0.5458, 0.5761, 0.3757, 0.4203, 0.5000],\n",
      "        [0.3418, 0.4611, 0.4542, 0.4239, 0.6243, 0.5797, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 300/600 [00:51<01:42,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.6806, 0.5544, 0.5714, 0.5770, 0.3645, 0.4020, 0.5000],\n",
      "        [0.3194, 0.4456, 0.4286, 0.4230, 0.6355, 0.5980, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 325/600 [01:00<01:41,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7031, 0.5743, 0.6069, 0.5799, 0.3523, 0.3837, 0.5000],\n",
      "        [0.2969, 0.4257, 0.3931, 0.4201, 0.6477, 0.6163, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 350/600 [01:10<01:39,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7252, 0.5949, 0.6471, 0.5843, 0.3392, 0.3659, 0.5000],\n",
      "        [0.2748, 0.4051, 0.3529, 0.4157, 0.6608, 0.6341, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 375/600 [01:21<01:40,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7472, 0.6165, 0.6907, 0.5874, 0.3254, 0.3506, 0.5000],\n",
      "        [0.2528, 0.3835, 0.3093, 0.4126, 0.6746, 0.6494, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 400/600 [01:32<01:33,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7701, 0.6391, 0.7379, 0.5913, 0.3109, 0.3363, 0.5000],\n",
      "        [0.2299, 0.3609, 0.2621, 0.4087, 0.6891, 0.6637, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 425/600 [01:44<01:26,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7952, 0.6659, 0.7898, 0.5949, 0.2943, 0.3213, 0.5000],\n",
      "        [0.2048, 0.3341, 0.2102, 0.4051, 0.7057, 0.6787, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 450/600 [01:58<01:20,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.8228, 0.6981, 0.8483, 0.5968, 0.2753, 0.3066, 0.5000],\n",
      "        [0.1772, 0.3019, 0.1517, 0.4032, 0.7247, 0.6934, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 475/600 [02:11<01:12,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.8530, 0.7393, 0.9138, 0.5986, 0.2545, 0.2892, 0.5000],\n",
      "        [0.1470, 0.2607, 0.0862, 0.4014, 0.7455, 0.7108, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 500/600 [02:26<00:58,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.8874, 0.7917, 0.9845, 0.5991, 0.2305, 0.2750, 0.5000],\n",
      "        [0.1126, 0.2083, 0.0155, 0.4009, 0.7695, 0.7250, 0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 525/600 [02:41<00:45,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.9292,  0.8574,  1.0620,  0.5959,  0.2002,  0.2621,  0.5000],\n",
      "        [ 0.0708,  0.1426, -0.0620,  0.4041,  0.7998,  0.7379,  0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 550/600 [02:57<00:32,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.9832,  0.9375,  1.1466,  0.5777,  0.1621,  0.2613,  0.5000],\n",
      "        [ 0.0168,  0.0625, -0.1466,  0.4223,  0.8379,  0.7387,  0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 575/600 [03:14<00:17,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.0552,  1.0308,  1.2393,  0.5156,  0.1134,  0.3046,  0.5000],\n",
      "        [-0.0552, -0.0308, -0.2393,  0.4844,  0.8866,  0.6954,  0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [03:32<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "[Parameter containing:\n",
      "tensor([[ 1.1483,  1.1354,  1.3413,  0.4089,  0.0471,  0.4004,  0.5000],\n",
      "        [-0.1483, -0.1354, -0.3413,  0.5911,  0.9529,  0.5996,  0.5000]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "A_1=Agent(env.observation_space,env.action_space)\n",
    "A_1.env_model.set_start_state()\n",
    "D_real.flush_all()\n",
    "init_params=torch.ones(env.action_space.n,env.observation_space.n)/env.action_space.n\n",
    "for param in A_1.model.fc1.parameters():\n",
    "    param.data = nn.parameter.Parameter(init_params)\n",
    "print(\"Before\")\n",
    "print(list(A_1.model.fc1.parameters()))\n",
    "\n",
    "for epoch_id in tqdm(range(600)):\n",
    "    D_real=update_D_real(D_real,env,A_1,1)\n",
    "    if epoch_id%25==0:\n",
    "        print(list(A_1.model.fc1.parameters()))\n",
    "    for i in range(1):\n",
    "        A_1.mod_MBPO_train_2(D_real,0)\n",
    "        # A_1.train_(D_real)\n",
    "print(\"After\")\n",
    "print(list(A_1.model.fc1.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {0: 46.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 51.0}, (0, 1): {0: 30.0, 1: 19.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 0): {0: 66.0, 1: 61.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 1): {0: 1.0, 1: 44.0, 2: 58.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 0): {0: 1.0, 1: 73.0, 2: 84.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 1): {0: 1.0, 1: 1.0, 2: 54.0, 3: 61.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 0): {0: 1.0, 1: 1.0, 2: 64.0, 3: 82.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 63.0, 4: 73.0, 5: 1.0, 6: 1.0}, (4, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 65.0, 4: 57.0, 5: 1.0, 6: 1.0}, (4, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 49.0, 5: 53.0, 6: 1.0}, (5, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 30.0, 5: 29.0, 6: 1.0}, (5, 1): {0: 1.0, 1: 24.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 18.0}, (6, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (6, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A_1.env_model.terminal_state=env.observation_space.n-1\n",
    "print(A_1.env_model.state_action_to_state_dict)\n",
    "# print(A_1.env_model.state_action_to_reward_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expect_num_steps(policy_param,num_of_epochs,Agent):\n",
    "    \n",
    "    A1=Agent\n",
    "    for param in A1.model.fc1.parameters():\n",
    "        param.data = nn.parameter.Parameter(policy_param)\n",
    "    \n",
    "    horizon_len=20\n",
    "    \n",
    "    env.reset()\n",
    "    # print(list(A_1.model.fc1.parameters()))\n",
    "    \n",
    "    s_t_index=env._state.index\n",
    "\n",
    "    trajs=[]\n",
    "    # D_real.flush_all()\n",
    "\n",
    "    result=0\n",
    "\n",
    "    b_count=[]\n",
    "    for traj_id in range(num_of_epochs):\n",
    "        env.reset()\n",
    "        # display_env()\n",
    "        s_t_index=env._state.index\n",
    "        \n",
    "        states=[]\n",
    "        log_probs=[]\n",
    "        rewards=[]\n",
    "        actions=[]\n",
    "        nstates=[]\n",
    "        \n",
    "        for t in range(horizon_len):\n",
    "            \n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            a_t, log_prob = A1.action(s_t)\n",
    "            ns_t_index, r_t, done, _ = env.step(a_t.numpy()[0][0])\n",
    "            \n",
    "            states.append(s_t_index)\n",
    "            actions.append(a_t)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(r_t)\n",
    "            nstates.append(ns_t_index)\n",
    "            s_t_index=ns_t_index\n",
    "            if done:\n",
    "                break\n",
    "            b_count.append(len(rewards))   \n",
    "        D_real.push(states, actions, rewards,nstates, log_probs)\n",
    "    \n",
    "    return sum(b_count)/len(b_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params=torch.Tensor([[ 1.1483,  1.1354,  1.3413,  0.4089,  0.0471,  0.4004,  0.5000],\n",
    "        [-0.1483, -0.1354, -0.3413,  0.5911,  0.9529,  0.5996,  0.5000]])\n",
    "init_para=init_params.numpy()\n",
    "np.savetxt(\"./experiment_new/alpha_variation/dummy_exp.csv\", init_para, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.748645913203497\n"
     ]
    }
   ],
   "source": [
    "my_data = np.genfromtxt(\"./experiment_new/alpha_variation/dummy_exp.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "rew_with_good=expect_num_steps(init_params.float(),10000,A_1)\n",
    "print(rew_with_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {0: 37.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 42.0}, (0, 1): {0: 41.0, 1: 38.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 0): {0: 66.0, 1: 64.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (1, 1): {0: 1.0, 1: 72.0, 2: 58.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 0): {0: 1.0, 1: 50.0, 2: 49.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (2, 1): {0: 1.0, 1: 1.0, 2: 63.0, 3: 73.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 0): {0: 1.0, 1: 1.0, 2: 60.0, 3: 51.0, 4: 1.0, 5: 1.0, 6: 1.0}, (3, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 78.0, 4: 79.0, 5: 1.0, 6: 1.0}, (4, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 57.0, 4: 41.0, 5: 1.0, 6: 1.0}, (4, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 48.0, 5: 54.0, 6: 1.0}, (5, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 24.0, 5: 21.0, 6: 1.0}, (5, 1): {0: 1.0, 1: 26.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 26.0}, (6, 0): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}, (6, 1): {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0}}\n"
     ]
    }
   ],
   "source": [
    "print(A_1.env_model.state_action_to_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(\"After\")\n",
    "# A_1.env_model.terminal_state=env.observation_space.n-1\n",
    "print(A_1.env_model.state_action_to_state_dict)\n",
    "# print(A_1.env_model.state_action_to_reward_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real=update_D_real(D_real,env,A_1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "A_1.env_model.update_param_given_epi(D_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_onehot(id):\n",
    "    id=torch.tensor(id)\n",
    "    s_t = torch.nn.functional.one_hot(id,num_classes=7)\n",
    "    s_t=s_t.type(torch.FloatTensor)\n",
    "    \n",
    "    return s_t\n",
    "\n",
    "def onehot_to_index(self,x):\n",
    "    id= torch.argmax(x, dim=1)\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "A_1.MBPO_train_2(D_real,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
