{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "\n",
    "from blackhc.mdp import dsl\n",
    "from blackhc import mdp\n",
    "import time\n",
    "\n",
    "from blackhc.mdp import lp\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import random\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    def flush_all(self):\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        return\n",
    "\n",
    "    def push(self, state, action, reward, next_state,policy):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state,policy)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def push_batch(self, batch):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            append_len = min(self.capacity - len(self.buffer), len(batch))\n",
    "            self.buffer.extend([None] * append_len)\n",
    "\n",
    "        if self.position + len(batch) < self.capacity:\n",
    "            self.buffer[self.position : self.position + len(batch)] = batch\n",
    "            self.position += len(batch)\n",
    "        else:\n",
    "            self.buffer[self.position : len(self.buffer)] = batch[:len(self.buffer) - self.position]\n",
    "            self.buffer[:len(batch) - len(self.buffer) + self.position] = batch[len(self.buffer) - self.position:]\n",
    "            self.position = len(batch) - len(self.buffer) + self.position\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        batch = random.sample(self.buffer, int(batch_size))\n",
    "        state, action, reward, next_state,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state,policy\n",
    "\n",
    "    def sample_all_batch(self, batch_size):\n",
    "        idxes = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = list(itemgetter(*idxes)(self.buffer))\n",
    "        state, action, reward, next_state,policy = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state,policy\n",
    "\n",
    "    def return_all(self):\n",
    "        return self.buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_env:\n",
    "    # initialize\n",
    "    def __init__(self,horizon_length,k):\n",
    "        \n",
    "        self.prev_state =None\n",
    "        self.curr_state =None\n",
    "           \n",
    "        self.state_list=list()\n",
    "        self.action_list=list()\n",
    "        self.state_to_action_map=dict()\n",
    "\n",
    "        \n",
    "        self.P=defaultdict()\n",
    "        self.R=defaultdict()\n",
    "        \n",
    "        self.horizon_len=horizon_length\n",
    "        self.terminal_state=None\n",
    "        self.k=k\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.state_list=list()\n",
    "        self.action_list=list()\n",
    "        self.state_to_action_map=dict()\n",
    "\n",
    "        \n",
    "        self.P=defaultdict()\n",
    "        self.R=defaultdict()\n",
    "        \n",
    "    # Parameter Estimation \n",
    "    \n",
    "    def update_param_given_epi(self,D_real):\n",
    "        \n",
    "        episodes=D_real.buffer\n",
    "        \n",
    "\n",
    "        # following SARSA format\n",
    "        for epi_id in range(len(episodes)):\n",
    "    \n",
    "            t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(episodes[epi_id])\n",
    "            i=0 \n",
    "            while i<len(t_states):\n",
    "                \n",
    "                # updating the list of states\n",
    "                \n",
    "                if any([torch.equal(x,t_states[i]) for x in self.state_list])!=True:\n",
    "                    self.state_list.append(t_states[i])\n",
    "                    self.state_to_action_map.update({t_states[i]:[]})\n",
    "                \n",
    "                \n",
    "                \n",
    "                # updating the list of actions\n",
    "            \n",
    "            \n",
    "\n",
    "                if any([torch.equal(x,t_actions[i]) for x in self.action_list])!=True:\n",
    "                    self.action_list.append(t_actions[i])\n",
    "                    \n",
    "                if any([torch.equal(x,t_actions[i]) for x in self.state_to_action_map[self.smooth_check(self.state_to_action_map,t_states[i])]])!=True:\n",
    "                    self.state_to_action_map[self.smooth_check(self.state_to_action_map,t_states[i])].append(t_actions[i])\n",
    "                \n",
    "                # # update state,action to next state count map\n",
    "                \n",
    "                tru_tup,flag=self.double_smooth_check(self.P,(t_states[i],t_actions[i]))\n",
    "                if flag!=True:\n",
    "                    self.P[(t_states[i],t_actions[i])]={t_nstates[i]:1}\n",
    "                    self.R[(t_states[i],t_actions[i])]={t_nstates[i]:t_rewards[i]}\n",
    "                    \n",
    "                else:\n",
    "                    if any([torch.equal(x,t_nstates[i]) for x in self.P[tru_tup].keys()])!=True:\n",
    "                        self.P[tru_tup].update({t_nstates[i]:1})\n",
    "                        self.R[tru_tup].update({t_nstates[i]:t_rewards[i]})\n",
    "                        \n",
    "                    else:\n",
    "                        sec_tup=self.smooth_check(self.P[tru_tup],t_nstates[i])\n",
    "                        self.P[tru_tup][sec_tup]+=1\n",
    "                                    \n",
    "                i+=1  \n",
    "                \n",
    "            if self.terminal_state is None and i<self.horizon_len:\n",
    "                self.terminal_state=t_nstates[i-1]\n",
    "                self.state_list.append(t_nstates[i-1])\n",
    "        return \n",
    "    \n",
    "    # Support functions\n",
    "    def double_smooth_check(self,A,a):\n",
    "        \n",
    "        for ele in A.keys():\n",
    "            if torch.equal(a[0],ele[0]) and torch.equal(a[1],ele[1]):\n",
    "                return ele,True\n",
    "        return a,False\n",
    "    \n",
    "    def smooth_check(self,A,a):\n",
    "\n",
    "        for ele in A.keys():\n",
    "            if torch.eq(a,ele).all():\n",
    "                return ele\n",
    "        return a\n",
    "     \n",
    "    def cvt_axis(self,traj):\n",
    "        \n",
    "        t_states =[]\n",
    "        t_actions =[]\n",
    "        t_nstates =[]\n",
    "        t_rewards=[]\n",
    "        t_log_probs=[]\n",
    "        \n",
    "        for i in range(len(traj[0])):\n",
    "            t_states.append(traj[0][i])\n",
    "            t_actions.append(traj[1][i])\n",
    "            t_rewards.append(traj[2][i])\n",
    "            t_nstates.append(traj[3][i])\n",
    "            t_log_probs.append(traj[4][i])\n",
    "\n",
    "        return (t_states, t_actions, t_rewards,t_nstates,t_log_probs) \n",
    "    \n",
    "    def get_parameters(self):\n",
    "        print(\"\\nState list\")\n",
    "        print(self.state_list)\n",
    "        print(\"\\nAction list\")\n",
    "        print(self.action_list)\n",
    "        print(\"\\nState to action map\")\n",
    "        print(self.state_to_action_map)\n",
    "        print(\"\\nstate_action to next state\")\n",
    "        for x in self.P:\n",
    "            print(x)\n",
    "            print(self.P[x])\n",
    "        print(\"\\n state_action to reward map\")\n",
    "        for x in self.R:\n",
    "            print(x)\n",
    "            print(self.R[x])\n",
    "        return\n",
    "    \n",
    "    def Is_terminal_state(self,s_t):\n",
    "        if torch.equal(self.terminal_state,s_t):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def set_start_state(self):\n",
    "        if len(self.state_list)>0:\n",
    "            \n",
    "            p=[1]*len(self.state_list)\n",
    "            p=[x/len(self.state_list) for x in p]\n",
    "            s_t_index=np.random.choice(np.arange(len(self.state_list)),p=p)\n",
    "            s_t=self.state_list[s_t_index]\n",
    "            while torch.equal(s_t,self.terminal_state):\n",
    "                s_t_index=np.random.choice(np.arange(len(self.state_list)),p=p)\n",
    "                s_t=self.state_list[s_t_index]\n",
    "            self.curr_state=s_t\n",
    "        return\n",
    "    \n",
    "    def list_check_up(self,A,s_t):\n",
    "        for x in A:\n",
    "            if torch.equal(x,s_t):\n",
    "                return x\n",
    "        return None\n",
    "   \n",
    "    # Fake Data generation functions\n",
    "    \n",
    "    def step_v1(self,a_t):\n",
    "        \n",
    "        next_state=0\n",
    "        un_norm_distr=self.P[self.double_smooth_check(self.P,(self.curr_state,a_t))[0]]\n",
    "        norm_factor=sum(list(un_norm_distr.values()))\n",
    "        choices=list(un_norm_distr.keys())\n",
    "        p=[x/norm_factor for x in un_norm_distr.values()]\n",
    "        \n",
    "        next_state_id=np.random.choice(np.arange(len(choices)),p=p)\n",
    "        next_state=choices[next_state_id]\n",
    "        rew_dict=self.R[self.double_smooth_check(self.R,(self.curr_state,a_t))[0]]\n",
    "        next_state_repr=None\n",
    "        for x in rew_dict.keys():\n",
    "            if torch.equal(x,next_state):\n",
    "                next_state_repr=x\n",
    "                break\n",
    "        reward=rew_dict[next_state_repr]\n",
    "        \n",
    "        Is_done=False\n",
    "        if self.Is_terminal_state(next_state):\n",
    "            Is_done=True\n",
    "\n",
    "        return next_state,reward,Is_done,None\n",
    "# sample a state from D_real\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, input_layer,output_layer):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_layer, output_layer,bias=False)\n",
    "        self.fc2=nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        x=self.fc1(input_)\n",
    "        y=self.fc2(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self,observation_space,action_space,gamma=0.99,learning_rate=1e-3,horizon_len=20,k=10,fraction_of_real=0.5,batch_size=200):\n",
    "\n",
    "        self.model = Network(observation_space.n,action_space.n)\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "        self.horizon_len=horizon_len # Assuming we already know the horizon length\n",
    "        self.env_model =pred_env(self.horizon_len,k)\n",
    "        self.D_fake=ReplayMemory(capacity=10000)\n",
    "        self.fraction_of_real=fraction_of_real\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "\n",
    "    def init_env_model(self):\n",
    "        self.env_model.set_start_state()\n",
    "        self.D_fake.flush_all()\n",
    "    def reset(self):\n",
    "        init_weights(self.model)\n",
    "        self.env_model.reset()\n",
    "        self.D_fake.flush_all()\n",
    "\n",
    "    def action(self, state):\n",
    "        \n",
    "        probs = self.model(Variable(state))\n",
    "        action = probs.multinomial(1).data\n",
    "        prob = probs[:, action[0,0]].view(1, -1)\n",
    "        log_prob = prob.log()\n",
    "\n",
    "        return(action, log_prob)\n",
    "    \n",
    "    def update_D_fake(self,num_of_epi,start_state=None):\n",
    "        \n",
    "    \n",
    "        if start_state is None:\n",
    "            s_t=self.env_model.curr_state\n",
    "        else:\n",
    "            s_t=start_state\n",
    "\n",
    "        self.env_model.curr_state=s_t\n",
    "        \n",
    "        \n",
    "        result=[]\n",
    "        \n",
    "\n",
    "        trajs=[]\n",
    "\n",
    "        for traj_id in range(num_of_epi):\n",
    "            \n",
    "            if self.env_model.Is_terminal_state(self.env_model.curr_state):\n",
    "                self.env_model.set_start_state()\n",
    "            s_t=self.env_model.curr_state\n",
    "            \n",
    "            states=[]\n",
    "            log_probs=[]\n",
    "            rewards=[]\n",
    "            actions=[]\n",
    "            nstates=[]\n",
    "            \n",
    "            for t in range(self.env_model.k):\n",
    "                a_t, log_prob = self.action(s_t)\n",
    "                while True:\n",
    "                    rlt=self.env_model.list_check_up(self.env_model.state_to_action_map,s_t)\n",
    "                    if rlt is None:\n",
    "                        print(self.env_model.state_to_action_map)\n",
    "                        print(s_t)\n",
    "                        print(a_t)\n",
    "                        print(done)\n",
    "                        print(\"Pover\")\n",
    "                        return \n",
    "                    else:\n",
    "                        if any([torch.equal(a_t,x) for x in self.env_model.state_to_action_map[rlt]])!=True:\n",
    "                                a_t, log_prob = self.action(s_t)\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                ns_t, r_t, done, _ = self.env_model.step_v1(a_t)\n",
    "\n",
    "                states.append(s_t)\n",
    "                actions.append(a_t)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(r_t)\n",
    "                nstates.append(ns_t)\n",
    "                \n",
    "                s_t=ns_t\n",
    "                self.curr_state=ns_t\n",
    "                if done:\n",
    "                    break\n",
    "            self.D_fake.push(states, actions, rewards,nstates, log_probs)      \n",
    "        return \n",
    "       \n",
    "    def cvt_axis(self,trajs):\n",
    "        t_states = []\n",
    "        t_actions = []\n",
    "        t_rewards = []\n",
    "        t_nstates = []\n",
    "        t_log_probs = []\n",
    "\n",
    "        for traj in trajs:\n",
    "            t_states.append(traj[0])\n",
    "            t_actions.append(traj[1])\n",
    "            t_rewards.append(traj[2])\n",
    "            t_nstates.append(traj[3])\n",
    "            t_log_probs.append(traj[4])\n",
    "\n",
    "        return (t_states, t_actions, t_rewards,t_states,t_log_probs)\n",
    "    \n",
    "    def reward_to_value(self,t_rewards, gamma):\n",
    "\n",
    "        t_Rs = []\n",
    "\n",
    "        for rewards in t_rewards:\n",
    "            Rs = []\n",
    "            R = torch.zeros(1, 1)\n",
    "\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                R = gamma * R + rewards[i]\n",
    "                Rs.insert(0, R)\n",
    "            t_Rs.append(Rs)\n",
    "            \n",
    "        return(t_Rs)\n",
    "\n",
    "    def cal_log_prob(self, state, action):\n",
    "\n",
    "        probs = self.model(Variable(state))\n",
    "        prob = probs[:, action[0,0]].view(1, -1)\n",
    "        log_prob = prob.log()\n",
    "\n",
    "        return(log_prob)\n",
    "    \n",
    "    def MBPO_train_1(self,D_real,mult_fcator=None):\n",
    "        \n",
    "        # Given D_real,and a multiplicative factor,will generate fake_data \n",
    "        # ST :len(D_fake)=multipl_factor*len(D_real)\n",
    "        \n",
    "        self.env_model.reset()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.init_env_model()\n",
    "        multiple_factor = (1-self.fraction_of_real)/self.fraction_of_real\n",
    "        if mult_fcator is not None:\n",
    "            multiple_factor=mult_fcator\n",
    "        self.update_D_fake(int(multiple_factor*D_real.position))\n",
    "        data_list=self.D_fake.buffer\n",
    "        \n",
    "        \n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(states[t], actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return\n",
    "      \n",
    "    def MBPO_train_2(self,D_real,fraction_of_real=None,mult_factor=1):\n",
    "        \n",
    "        # Given D_real,and the fraction of real to fake trajs,then train the policy on data comprising D_fake and D_real\n",
    "        # ST real_ratio  follows the value given\n",
    "        \n",
    "        self.env_model.reset()\n",
    "        self.env_model.update_param_given_epi(D_real)\n",
    "        self.init_env_model()\n",
    "        # mult_factor = (1-self.fraction_of_real)/self.fraction_of_real\n",
    "        \n",
    "        self.update_D_fake(int(mult_factor*D_real.position))\n",
    "        \n",
    "        frc_of_real=self.fraction_of_real\n",
    "        if fraction_of_real is not None:\n",
    "            frc_of_real=fraction_of_real\n",
    "        \n",
    "        self.batch_size=D_real.position\n",
    "        \n",
    "        num_of_real_epi=int(self.batch_size*frc_of_real)\n",
    "        num_of_fake_epi=self.batch_size-num_of_real_epi\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(self.D_fake.buffer),size=min([num_of_fake_epi,len(self.D_fake.buffer)]),replace=False)\n",
    "        fake_data_list=[self.D_fake.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        pos_list=np.random.choice(a=len(D_real.buffer),size=min([num_of_real_epi,len(D_real.buffer)]),replace=False)\n",
    "        real_data_list=[D_real.buffer[pos] for pos in pos_list]\n",
    "        \n",
    "        data_list=real_data_list+fake_data_list\n",
    "        \n",
    "        # print(len(real_data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(states[t], actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "        \n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "        return\n",
    "\n",
    "    def train_(self, D_real):\n",
    "        \n",
    "        # Pure policy gradient\n",
    "        \n",
    "        data_list=D_real.buffer\n",
    "        # print(len(data_list))\n",
    "        \n",
    "        t_states, t_actions, t_rewards,t_nstates,t_log_probs = self.cvt_axis(data_list)\n",
    "        t_Rs = self.reward_to_value(t_rewards, self.gamma)\n",
    "\n",
    "        Z = 0\n",
    "        b = 0\n",
    "        losses = []\n",
    "        Z_s = []\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            p_log_prob = 0\n",
    "            q_log_prob = 0\n",
    "            for t in range(len(Rs)):\n",
    "                p_log_prob += (self.cal_log_prob(states[t], actions[t])).data.numpy()\n",
    "                q_log_prob += log_probs[t].data.numpy()\n",
    "            Z_ = math.exp(p_log_prob) / math.exp(q_log_prob)\n",
    "            Z += Z_\n",
    "            Z_s.append(Z_)\n",
    "            b += Z_ * sum(Rs) / len(Rs)\n",
    "        b = b / Z\n",
    "\n",
    "\n",
    "        for (states, actions, Rs, log_probs) in zip(t_states, t_actions, t_Rs, t_log_probs):\n",
    "            loss = 0.\n",
    "\n",
    "            for t in range(len(Rs)):\n",
    "                loss = loss - (log_probs[t] * (Variable(Rs[t] - b).expand_as(log_probs[t]))).sum()\n",
    "\n",
    "            Z_ = Z_s.pop(0)\n",
    "            loss = loss / Z_\n",
    "            losses.append(loss)\n",
    "            \n",
    "        loss = sum(losses) / Z\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        utils.clip_grad_value_(self.model.parameters(),40)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.66666667, 2.22222222],\n",
       "       [2.22222222, 0.74074074],\n",
       "       [0.74074074, 0.33950617],\n",
       "       [0.33950617, 0.61728395],\n",
       "       [0.61728395, 1.85185185],\n",
       "       [1.85185185, 5.55555556],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  _multi_round_nmdp_simple():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(0) | dsl.reward(10)\n",
    "        start & A_0 > start * 10 | end\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_1 > start * 10 | end * 1 | S_1 * 1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(10)\n",
    "        S_1 & A_1 > start * 5 | end\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate()\n",
    "    \n",
    "def  _multi_round_nmdp_complex():\n",
    "    with dsl.new() as mdp:\n",
    "        # Write down the MDP dynamics here \n",
    "        \n",
    "        start = dsl.state()\n",
    "        S_1=dsl.state()\n",
    "        S_2=dsl.state()\n",
    "        S_3=dsl.state()\n",
    "        S_4=dsl.state()\n",
    "        S_5=dsl.state()\n",
    "        end = dsl.terminal_state()\n",
    "        \n",
    "        A_0=dsl.action()\n",
    "        A_1=dsl.action()\n",
    "\n",
    "        start & A_0 > dsl.reward(10) | dsl.reward(0)\n",
    "        start & A_0 > end * 1 | start\n",
    "        start & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        start & A_1 > start * 1 | S_1\n",
    "        \n",
    "        S_1 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_0 > S_1 * 1 | start\n",
    "        S_1 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_1 & A_1 > S_1 * 1 | S_2\n",
    "        \n",
    "        S_2 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_0 > S_2 * 1 | S_1\n",
    "        S_2 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_2 & A_1 > S_2 * 1 | S_3\n",
    "        \n",
    "        S_3 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_0 > S_3 * 1 | S_2\n",
    "        S_3 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_3 & A_1 > S_3 * 1 | S_4\n",
    "        \n",
    "        S_4 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_0 > S_4 * 1 | S_3\n",
    "        S_4 & A_1 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_4 & A_1 > S_4 * 1 | S_5\n",
    "        \n",
    "        S_5 & A_0 > dsl.reward(0) | dsl.reward(0)\n",
    "        S_5 & A_0 > S_5 * 1 | S_4\n",
    "        S_5 & A_1 > dsl.reward(10) | dsl.reward(0)\n",
    "        S_5 & A_1 > end * 1 | S_1\n",
    "        \n",
    "        dsl.discount(0.5)\n",
    "\n",
    "        return mdp.validate() \n",
    "\n",
    "MULTI_ROUND_NDMP = _multi_round_nmdp_complex()\n",
    "\n",
    "\n",
    "\n",
    "solver = lp.LinearProgramming(MULTI_ROUND_NDMP)\n",
    "solver.compute_q_table(max_iterations=10000, all_close=functools.partial(np.allclose, rtol=1e-10, atol=1e-10))\n",
    "# mdp.display_mdp(MULTI_ROUND_NDMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real=ReplayMemory(capacity=10000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling-based Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MDP-following openai gym structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MULTI_ROUND_NDMP.to_env()\n",
    "env.reset()\n",
    "A1=Agent(env.observation_space,env.action_space)\n",
    "A1.env_model.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_env():    \n",
    "    env.render()\n",
    "    env.render_widget.width=500\n",
    "    time.sleep(0.200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical-representation of the MDP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_epochs=1000\n",
    "# horizon_len=40\n",
    "\n",
    "num_of_epochs=30\n",
    "horizon_len=20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy policy gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# update D_real\n",
    "\n",
    "\n",
    "def update_D_real(D_real,env,Agent,num_of_epochs):\n",
    "    \n",
    "    A1=Agent\n",
    "    \n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    s_t_index=env._state.index\n",
    "    s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "    s_t=s_t.type(torch.FloatTensor)\n",
    "\n",
    "    trajs=[]\n",
    "    # D_real.flush_all()\n",
    "\n",
    "    result=0\n",
    "\n",
    "\n",
    "    for traj_id in range(num_of_epochs):\n",
    "        env.reset()\n",
    "        # display_env()\n",
    "        s_t_index=env._state.index\n",
    "        \n",
    "        states=[]\n",
    "        log_probs=[]\n",
    "        rewards=[]\n",
    "        actions=[]\n",
    "        nstates=[]\n",
    "        \n",
    "        for t in range(horizon_len):\n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            a_t, log_prob = A1.action(s_t)\n",
    "            ns_t, r_t, done, _ = env.step(a_t.numpy()[0][0])\n",
    "            # display_env()\n",
    "            if t!=0:\n",
    "                nstates.append(s_t)\n",
    "            states.append(s_t)\n",
    "            actions.append(a_t)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(r_t)\n",
    "            s_t_index=ns_t\n",
    "            if done:\n",
    "                break\n",
    "        # time.sleep(2)\n",
    "        s_t=F.one_hot(torch.tensor(s_t_index),num_classes=env.observation_space.n).unsqueeze(dim=0)\n",
    "        s_t=s_t.type(torch.FloatTensor)\n",
    "        nstates.append(s_t)\n",
    "        if traj_id==0:\n",
    "            result=sum(rewards)    \n",
    "        D_real.push(states, actions, rewards,nstates, log_probs)\n",
    "    \n",
    "    return D_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1560, -0.0975, -0.3777,  0.2764, -0.0925,  0.1440,  0.3418],\n",
       "         [ 0.3479,  0.1932,  0.3007,  0.2642, -0.2043,  0.3201,  0.3473]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(A1.model.fc1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_bench:\n",
    "    def __init__(self):\n",
    "        self.horizon_len=20     \n",
    "        self.num_of_real_epi=10\n",
    "        self.k=10\n",
    "        self.mult_factor=1.0\n",
    "        self.real_ratio=0.5\n",
    "        self.capacity=10000\n",
    "        self.batch_size=2000\n",
    "        self.learning_rate=0.001\n",
    "        self.gamma=0.99\n",
    "        \n",
    "        self.num_of_outerloop=100 # outer loop\n",
    "        self.num_of_innerloop=10 # inner loop\n",
    "        \n",
    "        self.D_real=ReplayMemory(self.capacity)\n",
    "        self.env=None\n",
    "        self.Agent=None\n",
    "    def init_play_ground(self,env):\n",
    "        self.env=env\n",
    "        self.env.reset()\n",
    "        self.A1=Agent(self.env.observation_space,self.env.action_space,gamma=self.gamma,learning_rate=self.learning_rate,horizon_len=self.horizon_len,k=self.k,fraction_of_real=self.real_ratio,batch_size=self.batch_size)\n",
    "        self.A1.reset()\n",
    "        \n",
    "    def reset_play_ground(self):\n",
    "        self.env.reset()\n",
    "        # self.A1.reset()     #######      !!!!!!!!!!!!!!!      ########\n",
    "        self.D_real.flush_all()\n",
    "        \n",
    "    def display_env(self):    \n",
    "        self.env.render()\n",
    "        self.env.render_widget.width=500\n",
    "        time.sleep(0.200)\n",
    "        \n",
    "    def update_D_real(self,num_of_epochs=None):\n",
    "    \n",
    "        num_of_episodes=self.num_of_real_epi\n",
    "        if num_of_epochs is not None:\n",
    "            num_of_episodes=num_of_epochs\n",
    "        \n",
    "        \n",
    "        self.env.reset()\n",
    "        \n",
    "        s_t_index=self.env._state.index\n",
    "        s_t=F.one_hot(torch.tensor(s_t_index),num_classes=self.env.observation_space.n).unsqueeze(dim=0)\n",
    "        s_t=s_t.type(torch.FloatTensor)\n",
    "\n",
    "        trajs=[]\n",
    "        # D_real.flush_all()\n",
    "\n",
    "        result=[]\n",
    "\n",
    "\n",
    "        for traj_id in range(num_of_episodes):\n",
    "            self.env.reset()\n",
    "            # display_env()\n",
    "            s_t_index=self.env._state.index\n",
    "            \n",
    "            states=[]\n",
    "            log_probs=[]\n",
    "            rewards=[]\n",
    "            actions=[]\n",
    "            nstates=[]\n",
    "        \n",
    "            for t in range(self.horizon_len):\n",
    "                s_t=F.one_hot(torch.tensor(s_t_index),num_classes=self.env.observation_space.n).unsqueeze(dim=0)\n",
    "                s_t=s_t.type(torch.FloatTensor)\n",
    "                a_t, log_prob = self.A1.action(s_t)\n",
    "                ns_t, r_t, done, _ = self.env.step(a_t.numpy()[0][0])\n",
    "                # display_env()\n",
    "                if t!=0:\n",
    "                    nstates.append(s_t)\n",
    "                states.append(s_t)\n",
    "                actions.append(a_t)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(r_t)\n",
    "                s_t_index=ns_t\n",
    "                if done:\n",
    "                    break\n",
    "            # time.sleep(2)\n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=self.env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            nstates.append(s_t)\n",
    "\n",
    "            result.append(self.reward_to_value(rewards,0.5)) \n",
    "            self.D_real.push(states, actions, rewards,nstates, log_probs)\n",
    "        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def reward_to_value(self,t_rewards, gamma):\n",
    "\n",
    "        Rs = []\n",
    "        R = torch.zeros(1, 1)\n",
    "        gamma=0.5\n",
    "        for i in reversed(range(len(t_rewards))):\n",
    "            R = gamma * R + t_rewards[i]\n",
    "            \n",
    "        return(R)\n",
    "    \n",
    "    def perform_pure_fake(self,mul_factor,init_params):\n",
    "        \n",
    "        # MBPO based agent \n",
    "\n",
    "        self.reset_play_ground()\n",
    "        result=[]\n",
    "        \n",
    "        for param in self.A1.model.fc1.parameters():\n",
    "            param.data = nn.parameter.Parameter(init_params)\n",
    "\n",
    "        print(\"\\nBefore training:\")\n",
    "\n",
    "        print(list(self.A1.model.fc1.parameters()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for x in tqdm(range(self.num_of_outerloop)):\n",
    "            result=result+self.update_D_real(num_of_epochs=10)\n",
    "            for i in range(self.num_of_innerloop):\n",
    "                self.A1.MBPO_train_1(self.D_real,mul_factor)\n",
    "                # A1.MBPO_train_2(D_real)\n",
    "                # A1.train_(D_real)\n",
    "                # pass\n",
    "        print(\"\\nAfter training:\")\n",
    "        print(list(self.A1.model.fc1.parameters()))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def perform_mixed_strategy(self,fraction,init_params,mult_factor=1):\n",
    "        \n",
    "        # MBPO based agent \n",
    "\n",
    "        self.reset_play_ground()\n",
    "        result=[]\n",
    "        \n",
    "        for param in self.A1.model.fc1.parameters():\n",
    "            param.data = nn.parameter.Parameter(init_params)\n",
    "        \n",
    "        print(\"\\nBefore training:\")\n",
    "\n",
    "        print(list(self.A1.model.fc1.parameters()))\n",
    "        \n",
    "        for x in tqdm(range(self.num_of_outerloop)):\n",
    "            result=result+self.update_D_real(num_of_epochs=10)\n",
    "            if x%25==0:\n",
    "                print(list(self.A1.model.fc1.parameters()))\n",
    "                \n",
    "            for i in range(self.num_of_innerloop):\n",
    "                # self.A1.MBPO_train_1(self.D_real)\n",
    "                self.A1.MBPO_train_2(self.D_real,fraction,mult_factor)\n",
    "                # self.A1.train_(D_real)\n",
    "                # pass\n",
    "        print(\"\\nAfter training:\")\n",
    "        print(list(self.A1.model.fc1.parameters()))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def perform_pure_real(self,init_params=None,outerloop=100):\n",
    "        \n",
    "        # MBPO based agent \n",
    "\n",
    "        self.reset_play_ground()\n",
    "        result=[]\n",
    "        if init_params is not None:\n",
    "            for param in self.A1.model.fc1.parameters():\n",
    "                param.data = nn.parameter.Parameter(init_params)\n",
    "        else:\n",
    "            init_weights(self.A1.model.fc1)\n",
    "\n",
    "        print(\"\\nBefore training:\")\n",
    "\n",
    "        print(list(self.A1.model.fc1.parameters()))\n",
    "        \n",
    "        self.num_of_outerloop=outerloop\n",
    "        \n",
    "        for x in tqdm(range(self.num_of_outerloop)):\n",
    "            result=result+self.update_D_real(num_of_epochs=10)\n",
    "            \n",
    "            if x%25==0:\n",
    "                print(list(self.A1.model.fc1.parameters()))\n",
    "\n",
    "            for i in range(self.num_of_innerloop):\n",
    "                # self.A1.MBPO_train_1(self.D_real)\n",
    "                # self.A1.MBPO_train_2(D_real)\n",
    "                self.A1.train_(self.D_real)\n",
    "                # pass\n",
    "        print(\"\\nAfter training:\")\n",
    "        print(list(self.A1.model.fc1.parameters()))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def Fraction_exp(self,list_of_fraction_of_real):\n",
    "        \n",
    "        result=[]\n",
    "        for real_fraction in list_of_fraction_of_real:\n",
    "            trajs=[]\n",
    "            for i in range(10):\n",
    "                trajs.append(self.perform_mixed_strategy(real_fraction))\n",
    "            result.append(trajs)\n",
    "            \n",
    "        # result=np.array(result)\n",
    "        A=np.array(result)\n",
    "        mean=A.mean(axis=1)\n",
    "        std=np.sqrt(A.var(axis=1))\n",
    "        UCB=np.add(mean,std)\n",
    "        MEAN=mean\n",
    "        LCB=np.subtract(mean,std)\n",
    "        \n",
    "        for i in range(len(list_of_fraction_of_real)):\n",
    "            plt.title(list_of_fraction_of_real[i])\n",
    "            plt.plot(MEAN[i],'-b', label='mean')\n",
    "            plt.plot(LCB[i],'-r',label=\"LCB\")\n",
    "            plt.plot(UCB[i],'-g',label=\"UCB\")\n",
    "            # fill the area with black color, opacity 0.15\n",
    "            plt.fill_between(list(range(len(MEAN[i]))), UCB[i],LCB[i], color=\"k\", alpha=0.15)\n",
    "            plt.xlabel(\"Time axis\")\n",
    "            plt.ylabel(\"Result\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "        return\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBPO based agent \n",
    "\n",
    "# D_real.flush_all()\n",
    "# init_weights(A1.model)\n",
    "\n",
    "# print(\"\\nBefore training:\")\n",
    "\n",
    "# print(list(A1.model.fc1.parameters()))\n",
    "\n",
    "# for x in tqdm(range(100)):\n",
    "#     D_real=update_D_real(D_real,env,A1,10)\n",
    "#     for i in range(10):\n",
    "#         # A1.MBPO_train_1(D_real)\n",
    "#         # A1.MBPO_train_2(D_real)\n",
    "#         A1.train_(D_real)\n",
    "#         # pass\n",
    "# print(list(A1.model.fc1.parameters()))\n",
    "# print(\"\\nAfter training:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_ground=Test_bench()    # create an instance of Test_bench\n",
    "env = MULTI_ROUND_NDMP.to_env() # initialize the environment\n",
    "play_ground.init_play_ground(env=env)   # initialize Playground\n",
    "play_ground.reset_play_ground() # setting to the Default state of Playground\n",
    "temp=copy.copy(list(play_ground.A1.model.fc1.parameters()))#\n",
    "# init_params=copy.copy(temp[0].data).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1741,  0.1458,  0.0439,  0.0518, -0.2410,  0.0314,  0.3139],\n",
       "        [-0.0708,  0.2966, -0.0441,  0.2373, -0.0035,  0.3455, -0.3437]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_params=temp\n",
    "# init_para=init_params[0].detach().numpy()\n",
    "# np.savetxt(\"./experiment/init_param_big.csv\", init_para, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.genfromtxt(\"./experiment/init_param_big.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m init_params\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_params' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0174, -0.1990, -0.0372,  0.3741,  0.0769,  0.0338,  0.3562],\n",
      "        [-0.1931,  0.2764, -0.0541, -0.0509,  0.2145, -0.2138,  0.1645]],\n",
      "       dtype=torch.float64, requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for param in play_ground.A1.model.fc1.parameters():\n",
    "    param.data = nn.parameter.Parameter(init_params)\n",
    "\n",
    "# init_weights(play_ground.A1.model.fc1)\n",
    "# for param in play_ground.A1.model.fc1.parameters():\n",
    "#     param=temp\n",
    "\n",
    "# init_weights(play_ground.A1.model.fc1)\n",
    "print(list(play_ground.A1.model.fc1.parameters()))\n",
    "# init_params=torch.Tensor([[-0.7012,  0.9787,  0.8131],\n",
    "#         [-0.4855,  0.4138, -0.2821]])\n",
    "# init_para=init_params.numpy()\n",
    "# np.savetxt(\"./experiment/init_param_small.csv\", init_para, delimiter=\",\")\n",
    "\n",
    "\n",
    "\n",
    "# init_params=torch.Tensor([[ 1.9763, -1.9540,  0.8131],\n",
    "#         [-3.1630,  3.3465, -0.2821]])\n",
    "# init_para=init_params.numpy()\n",
    "# np.savetxt(\"./experiment/after_param_small.csv\", init_para, delimiter=\",\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_params=torch.Tensor([[ 1.9763, -1.9540,  0.8131],\n",
    "#         [-3.1630,  3.3465, -0.2821]])\n",
    "# init_para=init_params.numpy()\n",
    "# np.savetxt(\"./experiment/after_param_small.csv\", init_para, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before training:\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0174, -0.1990, -0.0372,  0.3741,  0.0769,  0.0338,  0.3562],\n",
      "        [-0.1931,  0.2764, -0.0541, -0.0509,  0.2145, -0.2138,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:00<00:52, 19.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0174, -0.1990, -0.0372,  0.3741,  0.0769,  0.0338,  0.3562],\n",
      "        [-0.1931,  0.2764, -0.0541, -0.0509,  0.2145, -0.2138,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 25/1000 [00:05<05:30,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0417, -0.1772, -0.0216,  0.3635,  0.0472, -0.0048,  0.3562],\n",
      "        [-0.2174,  0.2546, -0.0697, -0.0404,  0.2443, -0.1751,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50/1000 [00:19<11:22,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0629, -0.1577,  0.0036,  0.3715,  0.0167, -0.0354,  0.3562],\n",
      "        [-0.2386,  0.2351, -0.0949, -0.0483,  0.2748, -0.1446,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 75/1000 [00:43<16:35,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0849, -0.1363,  0.0316,  0.3768, -0.0114, -0.0618,  0.3562],\n",
      "        [-0.2607,  0.2137, -0.1229, -0.0536,  0.3028, -0.1181,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [01:16<21:03,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.1081, -0.1138,  0.0634,  0.3826, -0.0397, -0.0887,  0.3562],\n",
      "        [-0.2838,  0.1912, -0.1547, -0.0595,  0.3311, -0.0913,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 125/1000 [01:57<25:29,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.1330, -0.0902,  0.0999,  0.3905, -0.0694, -0.1166,  0.3562],\n",
      "        [-0.3087,  0.1676, -0.1912, -0.0673,  0.3608, -0.0633,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [02:47<30:31,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.1597, -0.0651,  0.1431,  0.4072, -0.1023, -0.1463,  0.3562],\n",
      "        [-0.3354,  0.1425, -0.2344, -0.0841,  0.3937, -0.0337,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 175/1000 [03:45<33:53,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.8858e-01, -3.7383e-02,  1.9463e-01,  4.4271e-01, -1.4177e-01,\n",
      "         -1.7988e-01,  3.5616e-01],\n",
      "        [-3.6430e-01,  1.1478e-01, -2.8591e-01, -1.1955e-01,  4.3319e-01,\n",
      "         -5.1435e-05,  1.6450e-01]], requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [04:53<37:43,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2210, -0.0047,  0.2556,  0.5098, -0.1920, -0.2207,  0.3562],\n",
      "        [-0.3967,  0.0821, -0.3469, -0.1866,  0.4834,  0.0408,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 225/1000 [06:09<40:23,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2598,  0.0389,  0.3314,  0.6022, -0.2621, -0.2782,  0.3562],\n",
      "        [-0.4356,  0.0385, -0.4226, -0.2791,  0.5535,  0.0983,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 250/1000 [07:33<42:22,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.3146,  0.1068,  0.4254,  0.7049, -0.3562, -0.3646,  0.3562],\n",
      "        [-0.4903, -0.0294, -0.5167, -0.3818,  0.6476,  0.1847,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 275/1000 [09:06<45:51,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.4004,  0.2042,  0.5190,  0.8010, -0.4509, -0.4672,  0.3562],\n",
      "        [-0.5761, -0.1268, -0.6103, -0.4778,  0.7423,  0.2873,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/1000 [10:45<46:07,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.5020,  0.2907,  0.5918,  0.8747, -0.5243, -0.5498,  0.3562],\n",
      "        [-0.6777, -0.2133, -0.6831, -0.5515,  0.8157,  0.3698,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 325/1000 [12:32<47:38,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.5837,  0.3590,  0.6522,  0.9356, -0.5850, -0.6156,  0.3562],\n",
      "        [-0.7594, -0.2816, -0.7435, -0.6124,  0.8764,  0.4357,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 350/1000 [14:26<50:41,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.6494,  0.4171,  0.7054,  0.9892, -0.6385, -0.6723,  0.3562],\n",
      "        [-0.8251, -0.3397, -0.7967, -0.6660,  0.9299,  0.4923,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 375/1000 [16:24<50:06,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.7063,  0.4690,  0.7540,  1.0379, -0.6871, -0.7232,  0.3562],\n",
      "        [-0.8820, -0.3916, -0.8452, -0.7148,  0.9785,  0.5432,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [18:31<52:10,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.7574,  0.5166,  0.7991,  1.0833, -0.7324, -0.7701,  0.3562],\n",
      "        [-0.9332, -0.4392, -0.8904, -0.7601,  1.0238,  0.5902,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 425/1000 [20:41<50:16,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.8047,  0.5613,  0.8418,  1.1261, -0.7752, -0.8142,  0.3562],\n",
      "        [-0.9804, -0.4839, -0.9331, -0.8029,  1.0666,  0.6342,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 450/1000 [22:56<50:41,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.8491,  0.6035,  0.8824,  1.1668, -0.8159, -0.8560,  0.3562],\n",
      "        [-1.0248, -0.5261, -0.9737, -0.8437,  1.1073,  0.6760,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 475/1000 [25:18<50:08,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.8912,  0.6439,  0.9215,  1.2060, -0.8550, -0.8960,  0.3562],\n",
      "        [-1.0669, -0.5665, -1.0128, -0.8828,  1.1464,  0.7160,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [27:45<50:03,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.9315,  0.6828,  0.9592,  1.2438, -0.8928, -0.9345,  0.3562],\n",
      "        [-1.1072, -0.6054, -1.0505, -0.9206,  1.1842,  0.7546,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 525/1000 [30:17<48:50,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.9704,  0.7204,  0.9958,  1.2805, -0.9294, -0.9718,  0.3562],\n",
      "        [-1.1461, -0.6430, -1.0871, -0.9573,  1.2208,  0.7919,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 550/1000 [32:54<48:21,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.0080,  0.7569,  1.0315,  1.3162, -0.9651, -1.0081,  0.3562],\n",
      "        [-1.1837, -0.6795, -1.1228, -0.9930,  1.2565,  0.8282,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 575/1000 [35:37<47:09,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.0445,  0.7925,  1.0663,  1.3511, -1.0000, -1.0435,  0.3562],\n",
      "        [-1.2202, -0.7151, -1.1576, -1.0279,  1.2914,  0.8636,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 600/1000 [38:24<46:09,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.0802,  0.8273,  1.1004,  1.3852, -1.0341, -1.0781,  0.3562],\n",
      "        [-1.2559, -0.7499, -1.1917, -1.0621,  1.3255,  0.8982,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 625/1000 [41:13<42:20,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.1150,  0.8614,  1.1339,  1.4188, -1.0676, -1.1120,  0.3562],\n",
      "        [-1.2907, -0.7840, -1.2252, -1.0956,  1.3591,  0.9321,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 650/1000 [44:06<40:53,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.1492,  0.8948,  1.1668,  1.4517, -1.1006, -1.1453,  0.3562],\n",
      "        [-1.3249, -0.8174, -1.2581, -1.1286,  1.3920,  0.9654,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 675/1000 [47:04<39:05,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.1827,  0.9278,  1.1992,  1.4842, -1.1330, -1.1781,  0.3562],\n",
      "        [-1.3584, -0.8504, -1.2905, -1.1610,  1.4244,  0.9982,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 700/1000 [50:07<36:37,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.2157,  0.9602,  1.2312,  1.5162, -1.1650, -1.2104,  0.3562],\n",
      "        [-1.3914, -0.8828, -1.3225, -1.1930,  1.4564,  1.0305,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 725/1000 [53:13<34:07,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.2482,  0.9922,  1.2627,  1.5477, -1.1966, -1.2423,  0.3562],\n",
      "        [-1.4239, -0.9148, -1.3540, -1.2246,  1.4880,  1.0623,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 750/1000 [56:26<32:36,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.2802,  1.0237,  1.2939,  1.5790, -1.2277, -1.2737,  0.3562],\n",
      "        [-1.4560, -0.9463, -1.3852, -1.2558,  1.5192,  1.0938,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 775/1000 [59:42<29:33,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.3119,  1.0549,  1.3247,  1.6098, -1.2586, -1.3048,  0.3562],\n",
      "        [-1.4876, -0.9775, -1.4160, -1.2867,  1.5500,  1.1249,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 800/1000 [1:03:03<27:09,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.3432,  1.0858,  1.3553,  1.6404, -1.2891, -1.3356,  0.3562],\n",
      "        [-1.5189, -1.0084, -1.4465, -1.3172,  1.5806,  1.1557,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 825/1000 [1:06:42<25:30,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.3741,  1.1164,  1.3855,  1.6706, -1.3194, -1.3661,  0.3562],\n",
      "        [-1.5498, -1.0390, -1.4768, -1.3475,  1.6108,  1.1861,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 850/1000 [1:10:20<21:44,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.4047,  1.1466,  1.4155,  1.7006, -1.3494, -1.3963,  0.3562],\n",
      "        [-1.5805, -1.0692, -1.5067, -1.3775,  1.6408,  1.2163,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 875/1000 [2:24:44<1:33:08, 44.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.4351,  1.1766,  1.4452,  1.7304, -1.3791, -1.4262,  0.3562],\n",
      "        [-1.6108, -1.0992, -1.5365, -1.4072,  1.6705,  1.2463,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900/1000 [2:28:40<16:12,  9.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.4652,  1.2064,  1.4747,  1.7599, -1.4086, -1.4559,  0.3562],\n",
      "        [-1.6409, -1.1290, -1.5660, -1.4367,  1.7000,  1.2760,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 925/1000 [2:32:27<11:12,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.4950,  1.2360,  1.5040,  1.7892, -1.4379, -1.4854,  0.3562],\n",
      "        [-1.6707, -1.1586, -1.5953, -1.4660,  1.7294,  1.3054,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 950/1000 [2:36:36<08:13,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.5246,  1.2653,  1.5331,  1.8183, -1.4670, -1.5146,  0.3562],\n",
      "        [-1.7003, -1.1879, -1.6244, -1.4952,  1.7585,  1.3347,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 975/1000 [2:40:28<03:56,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 1.5540,  1.2944,  1.5620,  1.8472, -1.4960, -1.5437,  0.3562],\n",
      "        [-1.7297, -1.2170, -1.6533, -1.5241,  1.7874,  1.3638,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [2:44:24<00:00,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After training:\n",
      "[Parameter containing:\n",
      "tensor([[ 1.5832,  1.3234,  1.5907,  1.8760, -1.5247, -1.5726,  0.3562],\n",
      "        [-1.7589, -1.2460, -1.6820, -1.5528,  1.8161,  1.3927,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "my_data = np.genfromtxt(\"./experiment/init_param_big.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "play_ground.num_of_outerloop=1000\n",
    "play_ground.num_of_innerloop=1\n",
    "result=play_ground.perform_pure_real(init_params.float(),play_ground.num_of_outerloop)\n",
    "# play_ground.num_of_outerloop=10\n",
    "# result=play_ground.perform_mixed_strategy(1,init_params.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before training:\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0174, -0.1990, -0.0372,  0.3741,  0.0769,  0.0338,  0.3562],\n",
      "        [-0.1931,  0.2764, -0.0541, -0.0509,  0.2145, -0.2138,  0.1645]],\n",
      "       requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [00:28<17:44,  1.12s/it]"
     ]
    }
   ],
   "source": [
    "my_data = np.genfromtxt(\"./experiment/init_param_big.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "# result=play_ground.perform_pure_real(init_params.float(),100)\n",
    "play_ground.num_of_outerloop=1000\n",
    "play_ground.num_of_innerloop=1\n",
    "result=play_ground.perform_pure_fake(1,init_params.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params=torch.Tensor([[ 1.5832,  1.3234,  1.5907,  1.8760, -1.5247, -1.5726,  0.3562],\n",
    "        [-1.7589, -1.2460, -1.6820, -1.5528,  1.8161,  1.3927,  0.1645]])\n",
    "init_para=init_params.numpy()\n",
    "np.savetxt(\"./experiment/inner_outer_variation/pure_real_inner_1_outer_1000.csv\", init_para, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.genfromtxt(\"./experiment/init_param_small.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "result=play_ground.perform_pure_fake(1,init_params.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.genfromtxt(\"./experiment/init_param_small.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "result=play_ground.perform_mixed_strategy(1,init_params.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before training:\n",
      "[Parameter containing:\n",
      "tensor([[-0.7012,  0.9787,  0.8131],\n",
      "        [-0.4855,  0.4138, -0.2821]], requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "fract_list=np.arange(0,1.1,.1)\n",
    "prim_policy_list=[]\n",
    "for fract in tqdm(fract_list):\n",
    "    fract=np.round(fract,2)\n",
    "    my_data = np.genfromtxt(\"./experiment/init_param_small.csv\", delimiter=',')\n",
    "    init_params=torch.from_numpy(my_data)\n",
    "    result=play_ground.perform_mixed_strategy(fract,init_params.float())\n",
    "    prim_policy_list.append(list(play_ground.A1.model.fc1.parameters()))\n",
    "prim_policy_list=np.array(prim_policy_list)\n",
    "np.savetxt(\"./experiment/exp_rslt_1.csv\", prim_policy_list, delimiter=\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_policy_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fract_list=np.arange(0,1.1,.1)\n",
    "sec_policy_list=[]\n",
    "number_of_times=10\n",
    "\n",
    "for fract in tqdm(fract_list):\n",
    "    fract=np.round(fract,2)\n",
    "    policy_for_fract=[]\n",
    "    for i in range(number_of_times):\n",
    "        my_data = np.genfromtxt(\"./experiment/init_param_small.csv\", delimiter=',')\n",
    "        init_params=torch.from_numpy(my_data)\n",
    "        result=play_ground.perform_mixed_strategy(fract,init_params.float())\n",
    "        policy_for_fract.append(list(play_ground.A1.model.fc1.parameters()))\n",
    "    policy_for_fract=np.array(policy_for_fract)\n",
    "    sec_policy_list.append(policy_for_fract)\n",
    "sec_policy_list=np.array(sec_policy_list)\n",
    "np.savetxt(\"./experiment/exp_rslt_2.csv\", sec_policy_list, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_policy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.16666667, 7.52025463],\n",
       "       [4.49652778, 8.81944444],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_Q=solver.compute_q_table(max_iterations=10000, all_close=functools.partial(np.allclose, rtol=1e-10, atol=1e-10))\n",
    "np.savetxt(\"./experiment/true_Q.csv\", true_Q, delimiter=\",\")\n",
    "true_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.arange(0,1,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_array_with_a_policy(policy_param,num_of_episodes):\n",
    "        play_ground.env.reset()\n",
    "        \n",
    "        s_t_index=play_ground.env._state.index\n",
    "        s_t=F.one_hot(torch.tensor(s_t_index),num_classes=play_ground.env.observation_space.n).unsqueeze(dim=0)\n",
    "        s_t=s_t.type(torch.FloatTensor)\n",
    "\n",
    "        trajs=[]\n",
    "        # D_real.flush_all()\n",
    "\n",
    "        result=[]\n",
    "        \n",
    "        for param in play_ground.A1.model.fc1.parameters():\n",
    "            param.data = nn.parameter.Parameter(policy_param)\n",
    "        \n",
    "        \n",
    "        b_states=[]\n",
    "        b_log_probs=[]\n",
    "        b_rewards=[]\n",
    "        b_actions=[]\n",
    "        b_nstates=[]\n",
    "        b_count=[]\n",
    "        \n",
    "        \n",
    "        for traj_id in range(num_of_episodes):\n",
    "            play_ground.env.reset()\n",
    "            # display_env()\n",
    "            s_t_index=play_ground.env._state.index\n",
    "            \n",
    "            states=[]\n",
    "            log_probs=[]\n",
    "            rewards=[]\n",
    "            actions=[]\n",
    "            nstates=[]\n",
    "        \n",
    "            for t in range(100):\n",
    "                s_t=F.one_hot(torch.tensor(s_t_index),num_classes=play_ground.env.observation_space.n).unsqueeze(dim=0)\n",
    "                s_t=s_t.type(torch.FloatTensor)\n",
    "                a_t, log_prob = play_ground.A1.action(s_t)\n",
    "                ns_t, r_t, done, _ = play_ground.env.step(a_t.numpy()[0][0])\n",
    "                # display_env()\n",
    "                if t!=0:\n",
    "                    nstates.append(s_t)\n",
    "                states.append(s_t)\n",
    "                actions.append(a_t)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(r_t)\n",
    "                s_t_index=ns_t\n",
    "                if done:\n",
    "                    break\n",
    "            # time.sleep(2)\n",
    "            s_t=F.one_hot(torch.tensor(s_t_index),num_classes=play_ground.env.observation_space.n).unsqueeze(dim=0)\n",
    "            s_t=s_t.type(torch.FloatTensor)\n",
    "            nstates.append(s_t)\n",
    "            \n",
    "            b_states.append(states)\n",
    "            b_actions.append(actions)\n",
    "            b_rewards.append(rewards)\n",
    "            b_states.append(nstates)\n",
    "            b_count.append(len(rewards))\n",
    "        return b_states,b_actions,b_rewards,b_nstates,sum(b_count)/len(b_count)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain-Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward_array with bad parameters \n",
      "24.1017\n",
      "\n",
      "Reward_array with good parameters : fraction 0.4\n",
      "19.2233\n",
      "\n",
      "Reward_array with good parameters : fraction 0.4\n",
      "5.1448\n"
     ]
    }
   ],
   "source": [
    "my_data = np.genfromtxt(\"./experiment/init_param_big.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "rew_with_bad=reward_array_with_a_policy(init_params.float(),10000)\n",
    "print(\"Reward_array with bad parameters \")\n",
    "print(rew_with_bad[4])\n",
    "\n",
    "# my_data = np.genfromtxt(\"./experiment/alpha_variation/after_param_big_alpha_1.csv\", delimiter=',')\n",
    "# init_params=torch.from_numpy(my_data)\n",
    "# rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "# print(\"\\nReward_array with good parameters : fraction 1\")\n",
    "# print(rew_with_good[4])\n",
    "\n",
    "# my_data = np.genfromtxt(\"./experiment/alpha_variation/after_param_big_alpha_point_0.csv\", delimiter=',')\n",
    "# init_params=torch.from_numpy(my_data)\n",
    "# rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "# print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "# print(rew_with_good[4])\n",
    "\n",
    "\n",
    "# my_data = np.genfromtxt(\"./experiment/alpha_variation/after_param_big_alpha_point_0_v1.csv\", delimiter=',')\n",
    "# init_params=torch.from_numpy(my_data)\n",
    "# rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "# print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "# print(rew_with_good[4])\n",
    "\n",
    "# my_data = np.genfromtxt(\"./experiment/alpha_variation/after_param_big_alpha_point_0_v2.csv\", delimiter=',')\n",
    "# init_params=torch.from_numpy(my_data)\n",
    "# rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "# print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "# print(rew_with_good[4])\n",
    "\n",
    "# my_data = np.genfromtxt(\"./experiment/alpha_variation_big/after_param_big_alpha_point_0.csv\", delimiter=',')\n",
    "# init_params=torch.from_numpy(my_data)\n",
    "# rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "# print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "# print(rew_with_good[4])\n",
    "\n",
    "# my_data = np.genfromtxt(\"./experiment/alpha_variation_big/after_param_big_alpha_point_0_gain_1_epoch_150.csv\", delimiter=',')\n",
    "# init_params=torch.from_numpy(my_data)\n",
    "# rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "# print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "# print(rew_with_good[4])\n",
    "\n",
    "# my_data = np.genfromtxt(\"./experiment/alpha_variation_big/after_param_big_alpha_point_0_v2.csv\", delimiter=',')\n",
    "# init_params=torch.from_numpy(my_data)\n",
    "# rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "# print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "# print(rew_with_good[4])\n",
    "\n",
    "my_data = np.genfromtxt(\"./experiment/inner_outer_variation/pure_real_inner_1_outer_200.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "print(rew_with_good[4])\n",
    "\n",
    "my_data = np.genfromtxt(\"./experiment/inner_outer_variation/pure_real_inner_1_outer_1000.csv\", delimiter=',')\n",
    "init_params=torch.from_numpy(my_data)\n",
    "rew_with_good=reward_array_with_a_policy(init_params.float(),10000)\n",
    "print(\"\\nReward_array with good parameters : fraction 0.4\")\n",
    "print(rew_with_good[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]]), tensor([[1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "print(rew_with_good[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]])]\n"
     ]
    }
   ],
   "source": [
    "print(rew_with_good[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 10]\n"
     ]
    }
   ],
   "source": [
    "print(rew_with_good[2][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting episodes vs rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(result)), result)\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('episodes')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
